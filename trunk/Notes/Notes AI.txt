----
Notes AI
Artificial Intelligence,A Modern Approach (1st Ed. & 3rd Ed)
Stuart J. Russell and Peter Norvig
Prentice Hall, Englewood Cliffs, New Jersey
ISBN 0-13-103805-2 1995 (1st Edition)
ISBN 978-0-13-604259-4 2010 (3rd Edition)
----

----
Machine Learning Projects for .NET Developers
By: Mathias Brandewinder
Publisher: Apress
Pub. Date: July 6, 2015
Print ISBN-13: 978-1-4302-6767-6
Pages in Print Edition: 300
----

----
Machine Learning is all about structured data
test data sets available online
http://archive.ics.uci.edu/ml/
----

----
Mastering Predictive Analytics with R
By: Rui Miguel Forte
Publisher: Packt Publishing
Pub. Date: June 17, 2015
Web ISBN-13: 978-1-78398-281-3
Print ISBN-13: 978-1-78398-280-6
Pages in Print Edition: 414
----

----
Deep Learning
By: Josh Patterson; Adam Gibson
Publisher: O'Reilly Media, Inc.
Pub. Date: August 9, 2017
Print ISBN-13: 978-1-4919-1425-0
Pages in Print Edition: 532
----

----
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
----

----
AI is presented as a kind of API
 - in the broader use of the word its 
   a program plus some kind of ability 
   to perceive.
 - has a lot of technical words with 
   specific meanings
 - practical math ports used in AI
  - Hidden Markov Models: used in lang-rec being based 
    on existing math and allowed for training using 
	speech samples.
  - Bayesian network:  deals with probablity and 
    making decisions with uncertainty involved.
 - recent changes: the knowledge base from which 
   to work seems to be as, if not more, important
   to generating solutions than the algorithm used
   - in other words, worst algo with 100M training 
     samples does better than best algo with only 1M
 - entity is the thing which makes choices
----

----
Agent
 - is part of an environment
 - has some means of perceiving it
 - has some means of effecting it
 - must have some kind of utility for measuring success.
 - preception is the input, action the output
  - the relation between them is called 'mapping'
 - has autonomy, determines actions based on its
   own exp of perceptions (with some kind of useful 
    starting point)
 - since AI is bound up to our environment it is 
   considered both the program and the senses (e.g. cameras)
 - AI that exist only virtually are named 'software agents'
   or 'softbots' as a portmanteau)
 - the general framework for an agents starts with 
   its precepts, actions, goals and environment
 - text includes example of a taxi driver
    Precepts: Cameras, speedometer, GPS, sonar, mic
    Actions: steer, eccelerate, brake, talk to passenger
    Goals: safe, fast, legal, comfortable trip, profit
    Envrionment: roads, traffic, peds customers
 - precept-action is too simple (a.k.a. reflex-agent), 
   agents will also need to have some kind of 
   register for 'internal-state'
    - internal state requires 
	 (1) how the environment changes independent of the agent
	 (2) how the environment changes dependent on the agent's actions
 - in addition the agent needs some overall kind of vision to choose
   an action (a goal)
 - a more advanced agent will perceive a series of actions from actions
   having many possiable choices and choose the one to maximize goals
  - this becomes further complicated by probability.
----

----
Expressing State (Represent the Environment)
 - text divides expression of state into three modes of 
   increasing complexity
  - atomic: each state is indivisible, a sinlge fact
    like an enum
  - factored: each state is a fixed set of attributes (variables)
  - structured: is factored states which are related to each other
----

----
Properties of an environment
 - accessible (Fully observable) vs. inaccessible (partially observable): 
   the more accessible the less need for an internal state since 
   conditions may be resolved at any time.
 - deterministic vs nondeterministic(stochastic): where future environment state
   is purely a function of the current state plus agent's actions.
 - episodic vs. nonepisodic(sequential): exp is circular or continuous, where circular
   exp only matters, "this time around"
 - static vs. dynamic: where the environment can change while the agent 
   is thinking
 - discrete vs. continuous: when the percepts are clear and limited to 
   a number of moves or not.
 - single agent vs. multiagent: is the environment being effected by
   other agents.
 - known vs. unknown: where the outcome of an action is not even known
   and must be discovered.
   
----
  
----
Problem Solving
 - must first involve a end-goal
 - goal formulation is the first step in problem solving
 - problem formulation is the next step
 - the term 'search' is taking a problem as input and 
   returning a solution (sequence of actions) as output.
   - a successor function x, S(x) is the general form 
     of, given 'x', 'S(x)' is a set of reachable states.
   - 'state space' is a list of all possiable states 
   - goal-test takes a state and determines if it meets 
     a goal.
   - path cost takes into account that more than one sequence 
     achieves the goal but at different costs
   - a problem is then defined as a type having 
    (1) initial state
	(2) actions (successor functions)
	(3) goal-test
	(4) path-cost functions
    (5) transition model: results of those actions
   - additionaly there is a search-cost where trying to solve
     the problem is costly on its own
 - performing the actions is called execution
 - the general form is then 
    formulate, search, execute
 - agent then finds a new goal
 - a contingency problem is one where the state must be 
   re-eval'ed after each action resulting initialy in a 
   complex desicion tree.
 - exploration problem is when the agent doesn't know the 
   effects of its actions and must experiment to disover them
----
 
----
Search Strategies
 - are judged by the metrix
  (1) Completeness:  is the strategy guaranteed to find a solution
       there is one.
  (2) Time Complexity: how long does it take to find a solution
  (3) Space Complexity: how much memory does it need to perform a search
  (4) Optimality: does the strategy find the highest-quality solution when
       there are several different solutions.

 - there are two major groupings of search strategies
  (1) uninformed search: have no info about the number of steps or the
      path cost from the current state to the goal (aka Blind search). 
  (2) heuristic search (aka informed search): simply are better informed 
      and can resolve quicker
  
 - Branching Factor: the number of new states yielded for resolving any single state
  - 'b', variable for branching factor's max number of successors of any node
  - 'd' variable for the depth of the shallowest goal node
  - 'm' variable for max length of any path in the state space
  - time: measure of the number of nodes generated during a search
  - space: the max number of nodes stored in memory
 
 - there are six minor kinds of uninformed searches (aka blind search)
  (1) Breadth-first search: root node is fully expanded then its direct
      descendents are fully expanded followed by thier descendents and so on.
	 - in general form, nodes at depth 'd' are expanded prior to nodes at depth
       'd + 1'
     - is very expensive: at level one there is 'b', at two there is 'b^2' 
	   at three there is 'b^3'
	 - the max number of nodes expanded is 
	   1 + b + b^2 + b^3 + ... + b^d
	 - at 100 nodes per second with a cost of 100 bytes per node a depth of 14
	   would take 3500 years to resolve and 11111 terabytes of memory
  (2) Uniform cost search: like Breadth-first but always expands the least 
      costly node first
	 - the general form of node expansion cost is 'g(n)'
	 - as such breath-first search expansion cost is 'depth(n)'
	 - will find the cheapest solution if the path cost never decreases
	   'g(SUCCESSOR(n)) > g(n)' for every node
     - the frontier will be an priority queue (no native implementation 
        in .NET @4.6.1)
  (3) Depth-first search: expands a node down to a terminal level
     - upon terminal node goes back up only one and expands sibling
	 - only requires mem for current terminal node and upper, unexpanded 
	   siblings.
	 - general form of memory is branching factor * depth (b*d) as compared
	   to b^d mem for breadth first
  (4) Depth-limited search: avoids the overflow of depth-first by having a 
      cutoff
	  - can be improved by using the state-space's diameter as the cutoff
  (5) Iterative deepening search: is like Depth-limited except that the 
      cutoff is iterated (starting at 0)
	  - nodes at the bottom of the search tree are expanded only once
	  - those just above bottom are expanded twice and so on
	  - the root is expanded then d + 1 times
	   (d + 1)*1 + (d)*b + (d - 1)*b^2 + ... + 3b^(d-2) + 2b^(d-1) + 1b^d
	  - the sum proves that the cost of expanding nodes multiple times 
	    is not as much as expected (for d=10, breadth-first was 111111 and 
		 iterative-deepening is 123456)
		- the more the nodes the less the cost of multiple expansion
  (6) Bidirectional search: search both foward from initial state and backward
      from the goal and two searches meet, the solution is returned
	  - can be very tricky to implement, for example what are the states that 
	     are the predecessors of the checkmate goal in chess.
	  - some kind of manager is needed to know where the meet happens
	  
 - Comparison of uniform search 
          (1)      (2)      (3)              (4)           (5)      (6)
 Time     b^d      b^d      b^max-depth      b^cutoff      b^d      b^(d/2)
 Space    b^d      b^d      b*m              b*cutoff      b*d      b^(d/2)
 Optimal  true     true     false            false         true     true
 Complete true     true     false            true*         true     true
 
 - Avoiding repeated states
  - do not return from the state you just came from
  - manage paths and return only unique ones
  - do not generate any one state more than once
----

----
Constraint Satisfaction Search
 - aka CSP
 - state is a set of variables with values
 - goal-test specifies a set of contrants the values must obey
  - where each value must past some list of propositions
 - in general 
  - each variable (Vi) has a domain of (Di)
   - domain is the set of possiable values a variable may have
   - domains may be continuous or discrete
  - example is the 8-queens problem, where queen 1 is in the first 
    column of the board and queen two is in the second column of 
	the board.
    - the domains for both queen 1 and queen 2 is (1 .. 8)
	- the constraint could be represented as pairs of numbers
	  each representing a possiable attack 
	   - at column 1, row 1 we could attack column 2, row 1 and 
	     column 2, row 2, etc.
	   - solve for allowable pairs as ((1,3),(1,4),(1,5),...(2,4),(2,5),...)
	- any discrete CSP may be reduced to a binary CSP
 - general-purpose algorithm of discrete CSP
  - init of all variables unassigned
  - all ops will set variable values from discrete list
  - goal-test will determine if all variables are assigned 
    and all constraints are satisfied
 - a performance improvement happens here since constraints 
   - for example with the 8-queens, depth-first search will
     blindly continue to expand the nodes at (1,1) - which has
	 obviously already blown it.
 - backtracking search checks whether any constraint has been 
   violated by the variable assignments up to this point and pulls
   back in such a case
   - backtracking has shortcoming of getting to some end, finding 
     a violation and only going back a little when said violation 
	 implies a need to go back much further
 - foward checking will reduce the choices of the yet-to-be assigned
   variables since the current assigned variables imply obvious limits
   on the yet-to-be assigned variables possiable values.
   - this is a special case of "arc consistency"
   - arc consistency exhibits a form of constraint propagation
 - most-constraining-variable: in terms of heuristic the next 
   to receive a value should be the one involved in the largest 
   number of constraints on other yet-to-be-assigned variables
 - least-constraining-value: the value which should be used next 
   in assignment is a value that rules out the smallest number of values
   in variables connected to the current variable by constraints.
----

----
Informed Search Methods
 - these will involve using problem-specific knowledge 
 - when using a general-search the only place to apply 
   problem-specific knowledge is at the queuing function
  - knowledge to make a determination is called the 
    evaluation function
  - an evaluation function typically returns an int 
    which describes desirability
 - when a queue is order by desirability then its called
    best-first search
   - this is misnomer since such a ordered desirability would
     not be a search at all but simply a march
   - in reality eval functions sometimes get it wrong
 - desirability must be an estimate of the cost of the 
   path from a state to the closest goal
   
 - two common best-first are
  (1) Greedy Search: node whose state is judged to be closest to 
      the goal is expanded first.  
	- built on idea the best-first should try to minimize the estimated cost
	  to reach a goal
	- calc of estimated cost is called a heuristic function (denoted by 'h')
	- has requirement of h(n) = 0 means Goal!
	- prefers to take biggest bite possible out of remaining cost to reach 
	  a goal without considering if this particular choice is best in the long run
	 - this is not always the best solution
	 - susceptible to false starts, where choosing most-now can lead to a 
	   dead-end
	 - must check for repeating state or will get stuck going back-and-forth
	   in the same states while looking on at its goal
  (2) A*search: combines the greedy of biggest gain now with uniform-cost, or
      sum of cost up to now.
	  - so we have a possiable 'next node', we add how much the total cost would 
	    be to choose it to the greedy-gain from choosing it (or the amount we 
		reduced to reach our goal [h(n) = 0]) and then choose whichever node has the lowest 
		value.
	   - it works so long as the heuristic function (the op of a greedy search)
	     never over estimates the cost to the goal
	   - never over estimating the cost is called a 'admissible heuristic'
	  - the sum of uniform cost and heuristic cost is denoted as
	    f(n) = g(n) + h(n)
	  - whenever h(n) is an admissible heuristic the f(n) will always 
	    rise on each expanded node - this is called monotonicty
		- given two nodes 'parent-node' and 'child-node' and we calc the f(n)
		  such that 
		  f(parent-node) = g(parent-node) + h(parent-node) = 3 + 4 = 7
		  f(child-node)  = g(child-node) + h(child-node)   = 4 + 2 = 6
          
		 we see that f(child-node) is actually less than f(parent-node) so its
		 not a monotonicy h(n)
	  - we restore monotonicy by branching in such a case and taking the f(n) 
	    of the parent - this is called 'pathmax equation'
      - A* is optimally efficient for any given hueristic function, no other
        algorithm is guaranteed to expand fewer nodes than A*
----		

----
Heuristic Function:
 - has no particular formula for resolution (like how matrix alg walls 
   up at eigen vectors)
 - text uses 8-block puzzle (those cheap plastic puzzles with eight 
    tiles and one empty one)
 - tile in the corner could move 2 places
 - tile in the middle could move 4 places
 - the rest could move 3 places
  - that averages to a branch factor of 3
 - there are 9! possiable arrangements of the tiles
 - cannot move diagonial so calc city distance is called manhattan distance
 - effective branching factor is an algebra eq. solved for with guess of 
   total number of nodes expanded at some depth
  - at depth 5 with 52 nodes we find a solution then effective branching factor
    is solved using the formula from breadth-frist max possiable nodes
	N = 1 + b + b^2 + b^3 + ... + b^d
   52 = 1 + b + b^2 + b^3 + b^4 + b^5 
   - solve for 'b'
   - higher numbered results are always better and are stated
    has 'such-and-such heuristic f(x) dominates the other such-and-such f(x)'
 - still not clear how to derive a heuristic function - for distance it was 
   easy since we know the max straight-line distance and had something to subtract from
 - sometimes using a less restricted operator helps solve for the hueristic function
  - with the block problem tile movement (operator) was limited in direction and if
    the state was empty - if we loosen these restrictions its called 'relaxed problem'
  - when the operator is written down in formal syntax, relaxing it can be automatic
 - another trick is to have a catalog of heuristics and wrap them in a function that 
   returns the max.
 - yet another trick is to only solve for some subproblem within the whole and use 
   that heuristic for the whole.
----

----
Advanced Search 
 - Local Search: different kind of algorithm
  - current node: the base node from which Local Search ops
  - moves about amoung neighbors of current node
 - Optimization problems: "find the best state according 
   to an objective function"
   - objective function: the tautological thing which 
     defines the optimization
 - state-space landscape
 - [hill-climbing search]for max the local search will 
   expand all neighbor nodes and the one with the highest 
   value is selected and the current node is reassigned to it, 
   this repeats until a node is found which has no neighbors 
   which have a higher value than the current.
  - this could end up on only a local max, not the global max
  - by repeating the max-on-local search from random starting
    points the chances of getting stuck on a local high-point 
    are reduced.
  - max-on-local search cannot handle dream-like fractorial 
    environments 
 - [simulated annealing] on the reverse if looking for min 
   the text has helpful illustration of trying to get a 
   ping-pong ball into the deepest crevice on a bumpy surface.
  - just dropping it will lead to a local min
  - we can get it out of local min by shaking the surface
  - trick is to shake it just right so that shaking is 
    will get it out of any local min but not the global min
 - [local beam search]: like hill-climbing or its reverse but
    has (k) states instead of just one (current)
   - begins in (k) random states, if these states or thier 
     expanded neighbors are the solution its done.
   - otherwise take the one (k) best of the successors 
     and repeat
   - appears like just repeating the max-on-local search but
     its NOT since those are just (k) restarts, recall that
     the (k) best among all the successors is selected 
    - like (k) 49'ers spread across the prospect field,
      they all start at whatever location seems best (random).
    - if one of them calls out "gold" the others will leave thier 
      locations and gather around the bragger
    - obvious draw back is that this vein of gold is tiny
      compared to what one of them was about to hit in 
      a couple more pick-axe strikes.
----

----
Nodeterministic Search
 - Percepts become useful
 - in partially observable environment
  - percept narrow dow possiable states 
 - in nondeterministic environment
  - percepts tell which possible outcomes actually occurred
 - solution becomes a matter of contingency
  - called 'strategy'
 - transition model (idea was not present in 1st ed.)
  - is not longer a single but multiple results
  - solutions are in the form of nested branches (tree)

 - [interleaving] first taking action, then observes it
    the environment and computes the next action.
 - [exploration problem]: the agent does not know what states
   exist or what its actions do.
   - the reuslts of action (a) at state (s) can only be known
     by being at state (s) and doing action (a)
   - the text refers to 'online search' where 'online' means
     "process input data as the are received"
 - [competitive ratio]: the actual choosen path cost compared to
    its paragon
 - [irreversible state]: a state from which no action leads back
    to the previous state
   - no algorithm can avoid dead ends in all state spaces
 - [adversary argument]: imagined as an adversary constructing
    the state space as the agent explores it putting goals and
    dead-ends arbitrarily.
 - [safely explorable]: goal state is reachable from every 
    reachable state
----

----
 - Machine Learning is not as the name implies.
   It starts with some data set (the-bigger-the-better)
   which is divided into two parts, one part is for 
   training the other is for testing. 
   
 - framing the question
  - what is the input data from which we want to 
    extract information (model)?
  - what kind of model is most appropriate for this data?
  - what kind of answer would we like to elicit from new data 
    based on this model?
   
 - In supervised learning, each data item is tagged 
   with some kind of category which is the basis of 
   the learning. 
   - Each datam is a label-value hash
   - This is foreign to an imperative programmer because
     if you already know the answers (which is what these
     category tags are) there is nothing left to do.
   - It seems that whatever produced this data is crux
     of the whole effort so simply continue to use that
     for any other dataset.
   - This is were the idea of learning comes in since
     we are treating the program as a kind of student - 
     showing them the data with answers and focusing on
     how to arrive to those answers.
 - In unsupervised learning, you just have data with no
   tags.  
   - Idea is to find some pattern present therein
   - "all unintersting datasets are alike; each interesting
      dataset is interesting in its own way"
      - Leo Tolstoy
   - goal is to apply statistical & mathematical methods
     to systematically extract and summarize information 
     about the data.
   
 - Functional Lang is preferred since its most capable
   of dealing with large structured data where the 
   data is structured from primitive types.
   - These langs currency is not custom types with 
     interfaces but builtin data structures like array's
     maps, lists, sets and tuples.  These structures 
     may be combined into any kind graph.
 
----

----
 Discrete Analysis - Classifying Items
 - In practice the steps involve getting the data into 
   memory, dividing it between training and testing
   and then constructing the most rudimentary learning
   apparatus to quickly form a benchmark to work towards
   beating.
 - Implementation (imperative perspective)
    - Would involve some kind of type for the labels 
     (the distinct list the "answers").
   - Some kind of analyzer which would take data in 
     its raw form and return some kind of comparable 
     metric 
   - Some kind of classifier which would receive the 
     analyzer and some training data (where each item
     is both its label and data-value)
     - This classifier would then have some reusable 
       method which would take a data-value and 
       return a label.
   - The objective then becomes tweeking the analyzer
     to get better and better measures.
   - The crucial piece is what connects the metric to 
     the labels.
     - Vaguely takes of form of the probability of 
       such-and-such metric given this label.
     - Its applicable since the metric is expected 
       to be correlated to the label
     - Bayes' Theorem captures how strongly the metric
       is related to the label.
     - we solve by calc'ing Bayes' Theorem for each 
       label. The label with the highest metric wins
     - we are not setting some arbitrary level the 
       metric must reach to be considered this label
 - discretization:  when a metric is continuous feature 
   (like price or temp.) and we want to group into 
   discrete labels of (e.g. those above the avg and those
   below).

 - overfitting: is something that happens when it works
    great on the sample data it was trained with but
    does poorly when run against the population   
----   

----
Digression on Probability
 - Sample Space: the exhaustive set of all possible outcomes
  - is mutually exclusive, only one case can be the outcome 
    at-a-time
  - every possible outcome is present in the sample space
  - two 6-sided die has 36 discrete possible outcomes
  - upper-case greek omega is used as a symbol to represent 
    the sample space.
 
 - World: a really overloaded term, this is another name for
   'possible outcome' 
 
 - Probability Model: associates some ratio between 0 and 1 
   for each possible outcome in the sample space 
  - all associated ratio's within the model will sum to 1
  
 - Events: an overloaded term, here it means some grouping 
    of possible outcomes from the sample set which are 
    connected by some logical purpose 
    (e.g. outcomes where 2 6d add-up to 11)
   - the probability of an event is the sum of 
     its grouping (a.k.a. set)
   - e.g. 2 6d formally as
   P(Total = 11) = P((5,6)) + P((6,5))
                 = 1/36 + 1/36
                 = 1/18
                 
 - Unconditional Probabilities (a.k.a Prior Probabilities) 
   (a.k.a priors): is a kind of 'raw' probability of an event 
   absent any other information
    
 - Evidence: an overloaded term, here it means some 
   kind of information which has already been revealed
  - e.g. first of 2 6d already rolled a 5
 
 - Conditional Probability (a.k.a. Posterior Probability) 
  (a.k.a Posterior): is the probability given the evidence
  - this is where the '|' symbol starts being used, the term 
    following it is the evidence, so any term written with 
    it included is a conditional one.
  - bar reads as "given that" or just "given"
  - formally defined as: 
  P(A|B) = P(A -and  B) / P(B)
  
 - Product Rule: another form for writing a conditional 
   probability
  P(A -and B) = P(A|B)*P(B)
  P(A -and B) = P(B|A)*P(B)
 
 - Factored Representation: a way of representing a possible 
   outcome as a variable/value pairs
 
 - Random Variables: are the variables part of a factored rep.
  - these are often written as uppercase latin letters
  - is any kind of name given to something probable
  - e.g. P(Total = 11), 'Total' is the random variable
  - text implies this is synomous with the 
    overloaded 'Proposition'
  - can be combined using boolean ops (-not, -and, -or)
 
 - Domain: another overloaded term, here it means the possible
   values a random variable may take on
  - may be limited or infinite 
   - e.g. 6d is discrete and limited
   - rational numbers domain is infinite and continuous
   - integer numbers domain is infinite and discrete, etc.
  - represented typically in curly braces
  - e.g. 2 6d has a domain named 'Total' which is all the 
    various sums possible 2, 3, .. 12
  - e.g. Die_1 domain is {1,2,3,4,5,6}
  - boolean domain values are always written in lower-case 
    whole words (e.g. {true,false})
    
 - Probability Distribution: a vectorization of the probabilities
   of some random variable where ordinal is an assumed mapping
  - e.g. a random variable named 'Weather' has the following 
   P(Weather = sunny) = 0.6, P(Weather = rain) = 0.1, 
   P(Weather = cloudy) = 0.29, P(Weather = snow) = 0.01
   then its proability dist. is written as 
   P(Weather) = <0.6,0.1,0.29,0.01> (the AIMA text never 
    explains why its uses angle-brackets...)
    
 - Probability Density Function: 
  - a random variable which is infinite and\or continous 
    cannot have its probability dist. written as a vector
  - have to parameterize the random variable with something
    else to provide some kind of limit, typically its another
    variable
  - written by having the random variables given some value
   P(X = x) will be written as just P(x)
    
 - Conditional Distribution: a name for a probability dist. 
   which contains a 'given that' bar
   P(X|Y) would mean P(X = x[i] | Y = y[j]) for each 
   possible i,j pair
    
 - Joint Probability Distribution: a notation for dealing 
   with probabilities of multiple random variables
  - written using a comma delimiter (e.g. P(A,B)) which is
    the same thing as using the -and boolean op 
    (e.g. P(A,B) = P(A -and B))
  - forms a matrix of the cartesian product of each
    random variables probability dist.

 - Full Joint Probability Distribution: the cartesian product
   of all random variables in the the whole sample space
 
 - Inclusion-Exclusion Principle
  P(A -or B) = P(A) + P(B) - P(A -and B)
 
 - Negation Principle
  P(-not A) = 1 - P(A)
 
 - Probabilistic Inference: the calc of a posterior 
   probability given the full joint probability dist.
   
 - Bayesian(subjectivist) -vs- Frequentist (objectivist): 
   a frequentist has no expectation but expects the repeated 
   test will reveal the probability while bayesian starts 
   with some expectation and only adjust that expectation 
   when it proves false.

 - Principle of Indifference 
  (a.k.a. Principle of Insufficient Reason): when you don't
   know then assign equal probability to each
   
 - Marginal Probability: a row-sum of the full joint 
   probability dist.
  
 - Marginalization (a.k.a. summing-out): calc of marginal
   probability for each row.
  P(Y) = SUM(P(Y,z)) |z = 1 to Z
  - meaning the probability of Y is the sum of all possible
    combinations of values of set variables Z
 
 - Conditioning: another overloaded term, here it means 
   calc of the combo of marginal prob. and conditional prob.
  P(Y) = SUM(P(Y|z)*P(z)) |z = 1 to Z 
 
 - Normalization: the act of turning two or more probabilities,
   which stand in the correct relative portions to each other,
   to sum up to 1
  - is just the calc of each over the sum of all
   
 - Marginal Independece (a.k.a. absolute independence)
   (a.k.a. just 'independence'): when one random variable
   has absolutely nothing to do with another
  - formally defined as P(X|Y) = P(X)
  - useful in reducing the cartesian product of the full
    joint dist.
    
 - Bayes' Theorem: the probability of A given B will be 
   equal to the ratio of the probability of 
   B given A times the probability of A - over 
   the probability of B
   P(A|B) = (P(B|A) * P(A)) / P(B)
  - P(A) & P(B) will be the count of A or B over the total 
    count in the dataset 
   - so if we have a dataset of 5000 and count 250 of A
     then P(A) = 250 / 5000
   - P(A) and P(B) are the prob. of these things to the 
     whole while P(B|A) is the prob. of these things 
     to each other
   - algebra being
   P(A -or B) = P(A|B)*P(B)
   P(A -or B) = P(B|A)*P(A)
   P(B|A)*P(A) = P(A|B)*P(B)
   P(B|A) = (P(A|B)*P(B))/P(A)
 
 - Laplace Smoothing: will add 1 to the P(A) & P(B)
   so as in the above example of P(A) = 250 / 5000
   would be solved as P(A) = (1 + 250) / (1 + 5000)

 - Casual Direction -vs- Diagnostic Direction:
   is the idea that given the three values needed to
   solve Bayes theorem we consider P(B|A) as either
   B being an effect of A (casual) or B being the 
   cause of A (diagnostic)
   
 - Conditional Independence: springs from two or more 
   random variables both being caused by some third 
   random variable.  This makes the first two conditionally
   independent of each other
   
 - Separation: overloaded term, while the term conditional
   independence relates two variable, having the same cause,
   to each other, the causing random variable is the very 
   means of separating the conditional variables.
   
 - Naive Bayes Model: a way to simplify the full joint prob.
   dist. by acting naive and assuming one variable is the 
   cause of all the others.  This makes "all the others" 
   to be conditionally independent.

----

----
Bayesian Network
 - a concise way to represent any full joint probability dist.
 - is a combo of graph theory with prob. theory
 - model as a directed-graph
  - each node (a.k.a. points) (a.k.a. vertices) is a 
    random variable (discrete or continuous)
  - is acyclic so no loops or cycles
  - each nodes proability is P(x[i]|Parents(x[i]))
  - start by listing out all nodes
   - prefer obvious causes to be above their effects
  - iterate each node by drawing the edge(s) (a.k.a. links)
   - the given node is considered the effect 
     and are attempting to determine its cause (i.e. parents)
   - this is working in a casual direction rather than 
     a diagnostic one
  - record a conditional prob. table at each node 
  - if the graph is fully connected (a.k.a. complete) 
    then its the same a the full joint prob. dist.
  - continuous variables are often specified as a 
    dist. function (i.e. (mean, stdev))
 
 - Conditional Probability Table: the joint prob. dist.
   of just the parent nodes.
  - if its a kind of root node with no parent then its
    just that node's prob. dist. (i.e. a vector)
 
 - Conditional Case: one row in the conditional prob. table
 
 - Chain Rule: the means of calc P(x[i]|Parents(x[i]))
  - example of simple three random variables
  P(A,B,C) = P(A -and B -and C)
           = P(A | B -and C) * P(B -and C)
           = P(A | B -and C) * P(B|C) * P(C)
  - in abstract is 
  P(x[1],x[2],...x[n]) 
  = P(x[n] | x[n-1],...x[1])*P(x[n-1],...x[1]])
  = P(x[n] | x[n-1],...x[1])*P(x[n-1] | x[n-2],...x[n])
        ...P(x[2]|x[1])*P(x[1])
  - then for any given node the prob. is
  P(x[i]|x[i-1],...,x[1])
  = P(x[i]|Parents(x[i]))
 
 - Locally Structured (a.k.a. Sparse): the compactness 
   of a Bayesian network
  - typically is linear growth instead of exponential
  
 - Markov Blanket: term meaning a node is conditionally 
   independent of all other nodes given its parents,
   children and children's parents.
  
 - Deterministic Node: a kind of function node whose
   value can be calculated given its parents
 
 - Hybrid Bayesian Network: when both continuous and 
   discrete random variables are present in such a way
   that a discrete variable has a continuous parent or
   vice versa
  - the calc of such combos becomes each discrete variable
    "shifts" the distribution of the continuous variable
    around
    
 - Markov Chain: a formula related to getting the answer 
   to something via random sampling instead of brute-force calc
  - there is some kind of current state (x, x')
  - there is some number of steps (t)
   p[t](x) is the prob. of being in state x at time t
   p[t+1](x') is likewise for state x' at time t+1
   q(x->x') is probability of state going from x to x'
  p[t+1](x') = SUM(p[t](x)*q(x->x')) | for all x'
 
 - Stationary Distribution: a specific term related to 
   the Markov chain where p[t] = p[t+1]
  - this simplifies the above notation to just
  p(x') = SUM(p(x)*q(x->x')) | for all x'
  
 - Detailed Balance: stationary dist. depends this
  - its a kind of duplex calc where
  p(x)*q(x->x') = p(x')*q(x'->x) | for all x,x'
  
 - Gibbs Sampling: a kind of sampling where each
   variable is sampled conditionally on the current
   values of all other variables.
----

----
Fuzzy Logic
 - turns true\false into a range of 0 to 1
 - relates back to boolean ops using 
  T(A -and B) = min(T(A),T(B))
  T(A -or B) = max(T(A),T(B))
  T(-not A) = 1 - T(A)
 - gets werid quickly
 T(A) = 0.6, T(B) = 0.4
 T(A -and B) = 0.4
 T(A -and (-not A)) = min(0.6, (1 - 0.6)) 
 - which is also 0.4, what does that even mean
----

----
Shannon entropy
 - derived from Information Theory
 - a kind of measure for "how informative" a feature is
 - defined as:
 entropy(sample) = sum [ - p(x) * log p(x) ]
  - where p(x) is the proportion of x in the sample
  
 - given some features like:
 ["apple";"pear";"orange";"banana";"apple";"apple";"orange"]
 [   1   ;  1   ;   1    ;    1   ;   1   ;   2   ;    2   ]
  - the entropy value of the fruit names is 1.277034
    and the numbers entropy value of 0.598270
  - this indicates that the numbers are more predictive
    than the fruit names
 - the lower the entropy value the more predictive it is
----

----
All Numbers
 - in practice data is converted to some meaningful
   numerical representation so the whole corpus of 
   mathematical techniques may be applied.
 - similar in practice but uses regession analysis
 - base line is the average (or the line with 0 slope)
 - the metric here is least squares
  - sum of the square diff between each actual point
    and predicted point
 - tweaking the slope and intercept (theta0, theta1)
   is called Gradient Descent
 - extermum: where the derivitive slope is 0 
 - learning rate: x[next] = x - alpha * (derivitive g(x))
 
 - we are going to solve for theta0 and theta1 which gets
   closer to extermum
 - general equation form
  Y = θ[0]*X[0] + θ[1]*X[1] + θ[2]*X[2] + ... + θ[n]*X[n]
 - normal form regression 
 min cost(θ)=(1/N)*[Y-θ*Xͭ]*[Y-θ*Xͭ] 
  - the little (ͭ) means a matrix's transpose
  - is solved by
 θ=((Xᵀ*X)^-1)*Xᵀ*Y
 
 - colinearity: when any term can be expressed as a linear
   combination of the other features
 - linear combination: the construct of a set of terms each
   being multiplied by a constant and summing the results
   aX +bY where a and b are constants 
 - normalization: similar to meaning in DB's, means scaling
   all values to they are comparable
  (x_i - x_min) / (x_max - x_min)
----

----
K-Means Clustering
 - idea of instead of attempting to delimit a boundary 
   around an obvious cluster of data, we image there is
   some centerpoint and all the occurances are just
   imperfect variations of it.
  - the imagined centerpoints are called centroids
 
 - the algo works from the fact that as a centroids move 
   towards some concentration they must disassociate themselves
   from points which are not in the same direction as this
   concentration.  This allows for these latter points to 
   then be associated to another centroid.
 - by repeating the process the centroids become associated to 
   these at the cost of those and will eventually settle at
   some point where a concentration holds them.
 - the 'k' is arbitrary and must be supplied.
 
 -from the text (Brandewinder)
 clusterize
       1. initialize
               assign every observation an initial centroid ID of 0
               pick k distinct observations as the initial centroids 1 to k
       2. search
               assign each observation to the closest centroid
               if no assignment changes, done: we have good, stable centroids
               else
                      group observations assigned to the same cluster
                      reduce the observations to their average centroid
                      repeat with the new centroids
       3. search, starting from the initial centroids
 
 - would expect that after having been run the count of 
   items in each of the (k) clusters would be comparable
   - when this is not the case 3 things could be happening
   (1) there are not clusters whatsoever
   (2) we picked the wrong (k)
    - Residual Sum of Squares is the sum of distance
      of each observation from its centroid.
    - try different (k) values and take the one 
      that min.'s the Residual Sum of Squares
    - however, we want to limit complexity - if we
      chose a (k) equal to the number of observations
      it would be a perfect fit but meaningless.
    - some means of measuring complexity is required
      and a balance is needed between it an the RSS.
      - Akaike Information Criterion (AIC) uses
        2 * (number of features) * (k)
   (3) clusters don't form a round scatter-plot but 
       more vertical or horizontial lines - this means
       one of the axis need to be streched so the 
       clusters from a more rounded shape.
    - rescale is often performed with 
       x' = (x - min(x)) / (max(x) - min(x))
    - another form is to remove the magnitude
      to a proportion of itself using the max
     f0	    f1	    f2		MAX		f0/MAX	f1/MAX	f2/MAX
    1455	854	    54		1455	1.00	0.59	0.04
    2044	1255	134		2044	1.00	0.61	0.07
    90	    38	    8		90		1.00	0.42	0.09
----

----
Principal Component Analysis (PCA)
 - complement to k-means
 - for detecting some structure features
 - how features relate to each other
 - relies on the eigen decomposition of the dataset
   covariance matrix
 - expects features to be centered and on a similar scale
 - as name implies, is intended for reducing redundant features
 - builds on other relational concepts
  - Covariance:
   Cov(X,Y) = average((X-average(X)) * (Y-average(Y)))
   - if x and y move in the same direction together 
     then when x is above its average, y should be above
     its average.
   - has a problem of scale where its not clear 
     what a "high covariance" is without its data
  - Correlation:
   Corr(X,Y) = Cov(X,Y) / (stdDev(X) * stdDev(Y))
    -solves to-
   Corr(X,Y) = average((X-average(X))/stdDev(X) * (Y-average(Y))/stdDev(Y))
    - has property of going between -1.0 to 1.0
   z-score:  
    - rescale X to (X-average(X))/stdDev(X) is 
   - either resulting in zero doesn't mean 
     that no relation exists - it means the 
     relation, if any, is not a matter of moving
     up and down together (a perfect circle solves 
      to 0 but has a very definite relationship)
----   

----
Softmax Function
 - for probability distribution over K different possible outcomes
  - also known as Logistic regression
  - example in C#
  var z = new[] { 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0 };
  var z_exp = z.Select(i => Math.Pow(Math.E, i)).ToArray();
  var sum_z_exp = z_exp.Sum();
  var softmax = z_exp.Select(i => Math.Round(i / sum_z_exp, 3)).ToArray();
----

----
Deep Learning
 - where the machine is both finding mathematical optimization 
   and finding the features of raw data. 
  - the opposite would be Manual Designed Features, which 
    often takes a long time to draft
 - Neural Networks is a kind of deep learning
----

----
Deep Networks basics
 - core components
  - parameters: these are the matrix weights which 
     are gen'ed as small doubles
  - layers: the matrices themselves
  - activation functions: a function applied to the result 
     of some matrix dot-product
   - common hidden layer activation functions 
    - Sigmoid
    - Tanh
    - Hard tanh
    - ReLU (Rectified linear unit)
   - common output layer activation functions
    - single number, linear function
    - true\false, sigmoid function
    - classification, softmax
  - loss functions: the means of knowing the agreement between
      prediction and actual
    - regression
    - classification
    - reconstruction
  - optimization methods
   - two groups
    - First-order, uses Jacobian matrix (partial derivatives of 
       the loss-function with respect to each parameter)
    - Second-order, uses Hessian, which is the derivative of the Jacobian
  - hyperparameters
   - choices made before any calc
    - includes 
     - layer size
     - learning rate
     - regularization (way of not over-fitting)
     - activation functions
     - init weights 
     - loss functions
     - training batch size
     - normalization (making everything a number)
  
 - concepts
  - RBMs (Restricted Boltzmann Machines)
  - Autoencoders, main task is to reproduce its own input
     - useful to detect anomalies
 - architectures
  - UPNs
  - CNNs (image modeling)
  - Recurrent neural networks (sequence modeling)
  - Recursive neural networks
----
  
----
Support Vector Machines
 - an "off-the-self" kind of supervised learning
 - did very well head-to-head with more complex 
   specialized neural net (AIMA 18.11.1)
 
 - Maximum Margin Separator: a decision boundary with the largest 
   possible distance to example points.
  - loss is based on a separator that is farthest away from the 
    example seen so far.
  - is the mathematically complex form of  non-linear programming
    called, here, quadratic programming
 
 - Kernel Trick: data which cannont be divided nicely on a line
   can be divided if move up the dimensionality
  - AIMA Fig 18.31 explains this graphically
  - given some 2d scatter plot of white and black dots 
   - the black dots could clearly be circumscribed with a circle
   - circle is non-linear 
   - by imagining the 2d plot in 3d it would form a cone
   - the color of the cone would have the black dots up towards 
     its tip with the white dots forming its base
   - in 3d there is an obvious place where you could slice
     the cone separating the black from with dots (tip from base)
   - this slicing action is itself linear
----
 
 