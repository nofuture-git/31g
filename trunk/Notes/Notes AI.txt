----
Notes AI
Artificial Intelligence,A Modern Approach (1st Ed. & 3rd Ed)
Stuart J. Russell and Peter Norvig
Prentice Hall, Englewood Cliffs, New Jersey
ISBN 0-13-103805-2 1995 (1st Edition)
ISBN 978-0-13-604259-4 2010 (3rd Edition)
----

----
Machine Learning Projects for .NET Developers
By: Mathias Brandewinder
Publisher: Apress
Pub. Date: July 6, 2015
Print ISBN-13: 978-1-4302-6767-6
Pages in Print Edition: 300
----

----
Machine Learning is all about structured data
test data sets available online
http://archive.ics.uci.edu/ml/
----

----
Mastering Predictive Analytics with R
By: Rui Miguel Forte
Publisher: Packt Publishing
Pub. Date: June 17, 2015
Web ISBN-13: 978-1-78398-281-3
Print ISBN-13: 978-1-78398-280-6
Pages in Print Edition: 414
----

----
AI is presented as a kind of API
 - in the broader use of the word its 
   a program plus some kind of ability 
   to perceive.
 - has a lot of technical words with 
   specific meanings
 - practical math ports used in AI
  - Hidden Markov Models: used in lang-rec being based 
    on existing math and allowed for training using 
	speech samples.
  - Bayesian network:  deals with probablity and 
    making decisions with uncertainty involved.
 - recent changes: the knowledge base from which 
   to work seems to be as, if not more, important
   to generating solutions than the algorithm used
   - in other words, worst algo with 100M training 
     samples does better than best algo with only 1M
 - entity is the thing which makes choices
   
 - my own observation:
 The problem with the term A.I. is that it is target
 to all the investigations of L.W. - therefore, A.I.
 cannot be defined as a "thing"; rather, it is a catalog
 of computational practices whose effect is something
 we would call "intelligence" in ordinary language.
----

----
Agent
 - is part of an environment
 - has some means of perceiving it
 - has some means of effecting it
 - must have some kind of utility for measuring success.
 - preception is the input, action the output
  - the relation between them is called 'mapping'
 - has autonomy, determines actions based on its
   own exp of perceptions (with some kind of useful 
    starting point)
 - since AI is bound up to our environment it is 
   considered both the program and the senses (e.g. cameras)
 - AI that exist only virtually are named 'software agents'
   or 'softbots' as a portmanteau)
 - the general framework for an agents starts with 
   its precepts, actions, goals and environment
 - text includes example of a taxi driver
    Precepts: Cameras, speedometer, GPS, sonar, mic
    Actions: steer, eccelerate, brake, talk to passenger
    Goals: safe, fast, legal, comfortable trip, profit
    Envrionment: roads, traffic, peds customers
 - precept-action is too simple (a.k.a. reflex-agent), 
   agents will also need to have some kind of 
   register for 'internal-state'
    - internal state requires 
	 (1) how the environment changes independent of the agent
	 (2) how the environment changes dependent on the agent's actions
 - in addition the agent needs some overall kind of vision to choose
   an action (a goal)
 - a more advanced agent will perceive a series of actions from actions
   having many possiable choices and choose the one to maximize goals
  - this becomes further complicated by probability.
----

----
Expressing State (Represent the Environment)
 - text divides expression of state into three modes of 
   increasing complexity
  - atomic: each state is indivisible, a sinlge fact
    like an enum
  - factored: each state is a fixed set of attributes (variables)
  - structured: is factored states which are related to each other
----

----
Properties of an environment
 - accessible (Fully observable) vs. inaccessible (partially observable): 
   the more accessible the less need for an internal state since 
   conditions may be resolved at any time.
 - deterministic vs nondeterministic(stochastic): where future environment state
   is purely a function of the current state plus agent's actions.
 - episodic vs. nonepisodic(sequential): exp is circular or continuous, where circular
   exp only matters, "this time around"
 - static vs. dynamic: where the environment can change while the agent 
   is thinking
 - discrete vs. continuous: when the percepts are clear and limited to 
   a number of moves or not.
 - single agent vs. multiagent: is the environment being effected by
   other agents.
 - known vs. unknown: where the outcome of an action is not even known
   and must be discovered.
   
----
  
----
Problem Solving
 - must first involve a end-goal
 - goal formulation is the first step in problem solving
 - problem formulation is the next step
 - the term 'search' is taking a problem as input and 
   returning a solution (sequence of actions) as output.
   - a successor function x, S(x) is the general form 
     of, given 'x', 'S(x)' is a set of reachable states.
   - 'state space' is a list of all possiable states 
   - goal-test takes a state and determines if it meets 
     a goal.
   - path cost takes into account that more than one sequence 
     achieves the goal but at different costs
   - a problem is then defined as a type having 
    (1) initial state
	(2) actions (successor functions)
	(3) goal-test
	(4) path-cost functions
    (5) transition model: results of those actions
   - additionaly there is a search-cost where trying to solve
     the problem is costly on its own
 - performing the actions is called execution
 - the general form is then 
    formulate, search, execute
 - agent then finds a new goal
 - a contingency problem is one where the state must be 
   re-eval'ed after each action resulting initialy in a 
   complex desicion tree.
 - exploration problem is when the agent doesn't know the 
   effects of its actions and must experiment to disover them
----
 
----
Search Strategies
 - are judged by the metrix
  (1) Completeness:  is the strategy guaranteed to find a solution
       there is one.
  (2) Time Complexity: how long does it take to find a solution
  (3) Space Complexity: how much memory does it need to perform a search
  (4) Optimality: does the strategy find the highest-quality solution when
       there are several different solutions.

 - there are two major groupings of search strategies
  (1) uninformed search: have no info about the number of steps or the
      path cost from the current state to the goal (aka Blind search). 
  (2) heuristic search (aka informed search): simply are better informed 
      and can resolve quicker
  
 - Branching Factor: the number of new states yielded for resolving any single state
  - 'b', variable for branching factor's max number of successors of any node
  - 'd' variable for the depth of the shallowest goal node
  - 'm' variable for max length of any path in the state space
  - time: measure of the number of nodes generated during a search
  - space: the max number of nodes stored in memory
 
 - there are six minor kinds of uninformed searches (aka blind search)
  (1) Breadth-first search: root node is fully expanded then its direct
      descendents are fully expanded followed by thier descendents and so on.
	 - in general form, nodes at depth 'd' are expanded prior to nodes at depth
       'd + 1'
     - is very expensive: at level one there is 'b', at two there is 'b^2' 
	   at three there is 'b^3'
	 - the max number of nodes expanded is 
	   1 + b + b^2 + b^3 + ... + b^d
	 - at 100 nodes per second with a cost of 100 bytes per node a depth of 14
	   would take 3500 years to resolve and 11111 terabytes of memory
  (2) Uniform cost search: like Breadth-first but always expands the least 
      costly node first
	 - the general form of node expansion cost is 'g(n)'
	 - as such breath-first search expansion cost is 'depth(n)'
	 - will find the cheapest solution if the path cost never decreases
	   'g(SUCCESSOR(n)) > g(n)' for every node
     - the frontier will be an priority queue (no native implementation 
        in .NET @4.6.1)
  (3) Depth-first search: expands a node down to a terminal level
     - upon terminal node goes back up only one and expands sibling
	 - only requires mem for current terminal node and upper, unexpanded 
	   siblings.
	 - general form of memory is branching factor * depth (b*d) as compared
	   to b^d mem for breadth first
  (4) Depth-limited search: avoids the overflow of depth-first by having a 
      cutoff
	  - can be improved by using the state-space's diameter as the cutoff
  (5) Iterative deepening search: is like Depth-limited except that the 
      cutoff is iterated (starting at 0)
	  - nodes at the bottom of the search tree are expanded only once
	  - those just above bottom are expanded twice and so on
	  - the root is expanded then d + 1 times
	   (d + 1)*1 + (d)*b + (d - 1)*b^2 + ... + 3b^(d-2) + 2b^(d-1) + 1b^d
	  - the sum proves that the cost of expanding nodes multiple times 
	    is not as much as expected (for d=10, breadth-first was 111111 and 
		 iterative-deepening is 123456)
		- the more the nodes the less the cost of multiple expansion
  (6) Bidirectional search: search both foward from initial state and backward
      from the goal and two searches meet, the solution is returned
	  - can be very tricky to implement, for example what are the states that 
	     are the predecessors of the checkmate goal in chess.
	  - some kind of manager is needed to know where the meet happens
	  
 - Comparison of uniform search 
          (1)      (2)      (3)              (4)           (5)      (6)
 Time     b^d      b^d      b^max-depth      b^cutoff      b^d      b^(d/2)
 Space    b^d      b^d      b*m              b*cutoff      b*d      b^(d/2)
 Optimal  true     true     false            false         true     true
 Complete true     true     false            true*         true     true
 
 - Avoiding repeated states
  - do not return from the state you just came from
  - manage paths and return only unique ones
  - do not generate any one state more than once
----

----
Constraint Satisfaction Search
 - aka CSP
 - state is a set of variables with values
 - goal-test specifies a set of contrants the values must obey
  - where each value must past some list of propositions
 - in general 
  - each variable (Vi) has a domain of (Di)
   - domain is the set of possiable values a variable may have
   - domains may be continuous or discrete
  - example is the 8-queens problem, where queen 1 is in the first 
    column of the board and queen two is in the second column of 
	the board.
    - the domains for both queen 1 and queen 2 is (1 .. 8)
	- the constraint could be represented as pairs of numbers
	  each representing a possiable attack 
	   - at column 1, row 1 we could attack column 2, row 1 and 
	     column 2, row 2, etc.
	   - solve for allowable pairs as ((1,3),(1,4),(1,5),...(2,4),(2,5),...)
	- any discrete CSP may be reduced to a binary CSP
 - general-purpose algorithm of discrete CSP
  - init of all variables unassigned
  - all ops will set variable values from discrete list
  - goal-test will determine if all variables are assigned 
    and all constraints are satisfied
 - a performance improvement happens here since constraints 
   - for example with the 8-queens, depth-first search will
     blindly continue to expand the nodes at (1,1) - which has
	 obviously already blown it.
 - backtracking search checks whether any constraint has been 
   violated by the variable assignments up to this point and pulls
   back in such a case
   - backtracking has shortcoming of getting to some end, finding 
     a violation and only going back a little when said violation 
	 implies a need to go back much further
 - foward checking will reduce the choices of the yet-to-be assigned
   variables since the current assigned variables imply obvious limits
   on the yet-to-be assigned variables possiable values.
   - this is a special case of "arc consistency"
   - arc consistency exhibits a form of constraint propagation
 - most-constraining-variable: in terms of heuristic the next 
   to receive a value should be the one involved in the largest 
   number of constraints on other yet-to-be-assigned variables
 - least-constraining-value: the value which should be used next 
   in assignment is a value that rules out the smallest number of values
   in variables connected to the current variable by constraints.
----

----
Informed Search Methods
 - these will involve using problem-specific knowledge 
 - when using a general-search the only place to apply 
   problem-specific knowledge is at the queuing function
  - knowledge to make a determination is called the 
    evaluation function
  - an evaluation function typically returns an int 
    which describes desirability
 - when a queue is order by desirability then its called
    best-first search
   - this is misnomer since such a ordered desirability would
     not be a search at all but simply a march
   - in reality eval functions sometimes get it wrong
 - desirability must be an estimate of the cost of the 
   path from a state to the closest goal
   
 - two common best-first are
  (1) Greedy Search: node whose state is judged to be closest to 
      the goal is expanded first.  
	- built on idea the best-first should try to minimize the estimated cost
	  to reach a goal
	- calc of estimated cost is called a heuristic function (denoted by 'h')
	- has requirement of h(n) = 0 means Goal!
	- prefers to take biggest bite possible out of remaining cost to reach 
	  a goal without considering if this particular choice is best in the long run
	 - this is not always the best solution
	 - susceptible to false starts, where choosing most-now can lead to a 
	   dead-end
	 - must check for repeating state or will get stuck going back-and-forth
	   in the same states while looking on at its goal
  (2) A*search: combines the greedy of biggest gain now with uniform-cost, or
      sum of cost up to now.
	  - so we have a possiable 'next node', we add how much the total cost would 
	    be to choose it to the greedy-gain from choosing it (or the amount we 
		reduced to reach our goal [h(n) = 0]) and then choose whichever node has the lowest 
		value.
	   - it works so long as the heuristic function (the op of a greedy search)
	     never over estimates the cost to the goal
	   - never over estimating the cost is called a 'admissible heuristic'
	  - the sum of uniform cost and heuristic cost is denoted as
	    f(n) = g(n) + h(n)
	  - whenever h(n) is an admissible heuristic the f(n) will always 
	    rise on each expanded node - this is called monotonicty
		- given two nodes 'parent-node' and 'child-node' and we calc the f(n)
		  such that 
		  f(parent-node) = g(parent-node) + h(parent-node) = 3 + 4 = 7
		  f(child-node)  = g(child-node) + h(child-node)   = 4 + 2 = 6
          
		 we see that f(child-node) is actually less than f(parent-node) so its
		 not a monotonicy h(n)
	  - we restore monotonicy by branching in such a case and taking the f(n) 
	    of the parent - this is called 'pathmax equation'
      - A* is optimally efficient for any given hueristic function, no other
        algorithm is guaranteed to expand fewer nodes than A*
----		

----
Heuristic Function:
 - has no particular formula for resolution (like how matrix alg walls 
   up at eigen vectors)
 - text uses 8-block puzzle (those cheap plastic puzzles with eight 
    tiles and one empty one)
 - tile in the corner could move 2 places
 - tile in the middle could move 4 places
 - the rest could move 3 places
  - that averages to a branch factor of 3
 - there are 9! possiable arrangements of the tiles
 - cannot move diagonial so calc city distance is called manhattan distance
 - effective branching factor is an algebra eq. solved for with guess of 
   total number of nodes expanded at some depth
  - at depth 5 with 52 nodes we find a solution then effective branching factor
    is solved using the formula from breadth-frist max possiable nodes
	N = 1 + b + b^2 + b^3 + ... + b^d
   52 = 1 + b + b^2 + b^3 + b^4 + b^5 
   - solve for 'b'
   - higher numbered results are always better and are stated
    has 'such-and-such heuristic f(x) dominates the other such-and-such f(x)'
 - still not clear how to derive a heuristic function - for distance it was 
   easy since we know the max straight-line distance and had something to subtract from
 - sometimes using a less restricted operator helps solve for the hueristic function
  - with the block problem tile movement (operator) was limited in direction and if
    the state was empty - if we loosen these restrictions its called 'relaxed problem'
  - when the operator is written down in formal syntax, relaxing it can be automatic
 - another trick is to have a catalog of heuristics and wrap them in a function that 
   returns the max.
 - yet another trick is to only solve for some subproblem within the whole and use 
   that heuristic for the whole.
----

----
Advanced Search 
 - Local Search: different kind of algorithm
  - current node: the base node from which Local Search ops
  - moves about amoung neighbors of current node
 - Optimization problems: "find the best state according 
   to an objective function"
   - objective function: the tautological thing which 
     defines the optimization
 - state-space landscape
 - [hill-climbing search]for max the local search will 
   expand all neighbor nodes and the one with the highest 
   value is selected and the current node is reassigned to it, 
   this repeats until a node is found which has no neighbors 
   which have a higher value than the current.
  - this could end up on only a local max, not the global max
  - by repeating the max-on-local search from random starting
    points the chances of getting stuck on a local high-point 
    are reduced.
  - max-on-local search cannot handle dream-like fractorial 
    environments 
 - [simulated annealing] on the reverse if looking for min 
   the text has helpful illustration of trying to get a 
   ping-pong ball into the deepest crevice on a bumpy surface.
  - just dropping it will lead to a local min
  - we can get it out of local min by shaking the surface
  - trick is to shake it just right so that shaking is 
    will get it out of any local min but not the global min
 - [local beam search]: like hill-climbing or its reverse but
    has (k) states instead of just one (current)
   - begins in (k) random states, if these states or thier 
     expanded neighbors are the solution its done.
   - otherwise take the one (k) best of the successors 
     and repeat
   - appears like just repeating the max-on-local search but
     its NOT since those are just (k) restarts, recall that
     the (k) best among all the successors is selected 
    - like (k) 49'ers spread across the prospect field,
      they all start at whatever location seems best (random).
    - if one of them calls out "gold" the others will leave thier 
      locations and gather around the bragger
    - obvious draw back is that this vein of gold is tiny
      compared to what one of them was about to hit in 
      a couple more pick-axe strikes.
----

----
Nodeterministic Search
 - Percepts become useful
 - in partially observable environment
  - percept narrow dow possiable states 
 - in nondeterministic environment
  - percepts tell which possible outcomes actually occurred
 - solution becomes a matter of contingency
  - called 'strategy'
 - transition model (idea was not present in 1st ed.)
  - is not longer a single but multiple results
  - solutions are in the form of nested branches (tree)

 - [interleaving] first taking action, then observes it
    the environment and computes the next action.
 - [exploration problem]: the agent does not know what states
   exist or what its actions do.
   - the reuslts of action (a) at state (s) can only be known
     by being at state (s) and doing action (a)
   - the text refers to 'online search' where 'online' means
     "process input data as the are received"
 - [competitive ratio]: the actual choosen path cost compared to
    its paragon
 - [irreversible state]: a state from which no action leads back
    to the previous state
   - no algorithm can avoid dead ends in all state spaces
 - [adversary argument]: imagined as an adversary constructing
    the state space as the agent explores it putting goals and
    dead-ends arbitrarily.
 - [safely explorable]: goal state is reachable from every 
    reachable state
----

----
 - Machine Learning is not as the name implies.
   It starts with some data set (the-bigger-the-better)
   which is divided into two parts, one part is for 
   training the other is for testing. 
   
 - In supervised learning, each data item is tagged 
   with some kind of category which is the basis of 
   the learning. 
   - Each datam is a label-value hash
   - This is foreign to an imperative programmer because
     if you already know the answers (which is what these
     category tags are) there is nothing left to do.
   - It seems that whatever produced this data is crux
     of the whole effort so simply continue to use that
     for any other dataset.
   - This is were the idea of learning comes in since
     we are treating the program as a kind of student - 
     showing them the data with answers and focusing on
     how to arrive to those answers.
 - In unsupervised learning, you just have data with no
   tags.  
   - Idea is to find some pattern present therein
   - "all unintersting datasets are alike; each interesting
      dataset is interesting in its own way"
      - Leo Tolstoy
   - goal is to apply statistical & mathematical methods
     to systematically extract and summarize information 
     about the data.
   
 - Functional Lang is preferred since its most capable
   of dealing with large structured data where the 
   data is structured from primitive types.
   - These langs currency is not custom types with 
     interfaces but builtin data structures like array's
     maps, lists, sets and tuples.  These structures 
     may be combined into any kind graph.
 
----

----
 Discrete Analysis - Classifying Items
 - In practice the steps involve getting the data into 
   memory, dividing it between training and testing
   and then constructing the most rudimentary learning
   apparatus to quickly form a benchmark to work towards
   beating.
 - Implementation (imperative perspective)
    - Would involve some kind of type for the labels 
     (the distinct list the "answers").
   - Some kind of analyzer which would take data in 
     its raw form and return some kind of comparable 
     metric 
   - Some kind of classifier which would receive the 
     analyzer and some training data (where each item
     is both its label and data-value)
     - This classifier would then have some reusable 
       method which would take a data-value and 
       return a label.
   - The objective then becomes tweeking the analyzer
     to get better and better measures.
   - The crucial piece is what connects the metric to 
     the labels.
     - Vaguely takes of form of the probability of 
       such-and-such metric given this label.
     - Its applicable since the metric is expected 
       to be correlated to the label
     - Bayes' Theorem captures how strongly the metric
       is related to the label.
     - we solve by calc'ing Bayes' Theorem for each 
       label. The label with the highest metric wins
     - we are not setting some arbitrary level the 
       metric must reach to be considered this label
 - discretization:  when a metric is continuous feature 
   (like price or temp.) and we want to group into 
   discrete labels of (e.g. those above the avg and those
   below).
----

----
Bayes' Theorem

 P(A|B) = (P(B|A) * P(A)) / P(B)
  - bar reads as "given that"
  "the probability of A given B will be 
  equal to the ratio of the probability of 
  B given A times the probability of A - over 
  the probability of B"
  
  - P(A) & P(B) will be the count of A or B over the total 
    count in the dataset 
   - so if we have a dataset of 5000 and count 250 of A
     then P(A) = 250 / 5000
   - P(A) and P(B) are the prob. of these things to the 
     whole while P(B|A) is the prob. of these things 
     to each other
     
Laplace Smoothing: will add 1 to the P(A) & P(B)
 so as in the above example of P(A) = 250 / 5000
 would be solved as P(A) = (1 + 250) / (1 + 5000)
----

----
Shannon entropy
 - derived from Information Theory
 - a kind of measure for "how informative" a feature is
 - defined as:
 entropy(sample) = sum [ - p(x) * log p(x) ]
  - where p(x) is the proportion of x in the sample
  
 - given some features like:
 ["apple";"pear";"orange";"banana";"apple";"apple";"orange"]
 [   1   ;  1   ;   1    ;    1   ;   1   ;   2   ;    2   ]
  - the entropy value of the fruit names is 1.277034
    and the numbers entropy value of 0.598270
  - this indicates that the numbers are more predictive
    than the fruit names
 - the lower the entropy value the more predictive it is
----

----
All Numbers
 - in practice data is converted to some meaningful
   numerical representation so the whole corpus of 
   mathematical techniques may be applied.
 - similar in practice but uses regession analysis
 - base line is the average (or the line with 0 slope)
 - the metric here is least squares
  - sum of the square diff between each actual point
    and predicted point
 - tweaking the slope and intercept (theta0, theta1)
   is called Gradient Descent
 - extermum: where the derivitive slope is 0 
 - learning rate: x[next] = x - alpha * (derivitive g(x))
 
 - we are going to solve for theta0 and theta1 which gets
   closer to extermum
 - general equation form
  Y = θ[0]*X[0] + θ[1]*X[1] + θ[2]*X[2] + ... + θ[n]*X[n]
 - normal form regression 
 min cost(θ)=(1/N)*[Y-θ*Xͭ]*[Y-θ*Xͭ] 
  - the little (ͭ) means a matrix's transpose
  - is solved by
 θ=((Xͭ*X)^-1)*Xͭ*Y
 
 - colinearity: when any term can be expressed as a linear
   combination of the other features
 - linear combination: the construct of a set of terms each
   being multiplied by a constant and summing the results
   aX +bY where a and b are constants 
 - normalization: similar to meaning in DB's, means scaling
   all values to they are comparable
  (x_i - x_min) / (x_max - x_min)
----

----
Digression on Statistics 
 - List of general concepts
  - Variance, how wide the obs are spead along an axis
  - Clustering, divide obs into groups
  - Correlation, measure strength and direction of two
    obs 
----

----
K-Means Clustering
 - idea of instead of attempting to delimit a boundary 
   around an obvious cluster of data, we image there is
   some centerpoint and all the occurances are just
   imperfect variations of it.
  - the imagined centerpoints are called centroids
 
 - the algo works from the fact that as a centroids move 
   towards some concentration they must disassociate themselves
   from points which are not in the same direction as this
   concentration.  This allows for these latter points to 
   then be associated to another centroid.
 - by repeating the process the centroids become associated to 
   these at the cost of those and will eventually settle at
   some point where a concentration holds them.
 - the 'k' is arbitrary and must be supplied.
 
 -from the text (Brandewinder)
 clusterize
       1. initialize
               assign every observation an initial centroid ID of 0
               pick k distinct observations as the initial centroids 1 to k
       2. search
               assign each observation to the closest centroid
               if no assignment changes, done: we have good, stable centroids
               else
                      group observations assigned to the same cluster
                      reduce the observations to their average centroid
                      repeat with the new centroids
       3. search, starting from the initial centroids
 
 - would expect that after having been run the count of 
   items in each of the (k) clusters would be comparable
   - when this is not the case 3 things could be happening
   (1) there are not clusters whatsoever
   (2) we picked the wrong (k)
    - Residual Sum of Squares is the sum of distance
      of each observation from its centroid.
    - try different (k) values and take the one 
      that min.'s the Residual Sum of Squares
    - however, we want to limit complexity - if we
      chose a (k) equal to the number of observations
      it would be a perfect fit but meaningless.
    - some means of measuring complexity is required
      and a balance is needed between it an the RSS.
      - Akaike Information Criterion (AIC) uses
        2 * (number of features) * (k)
   (3) clusters don't form a round scatter-plot but 
       more vertical or horizontial lines - this means
       one of the axis need to be streched so the 
       clusters from a more rounded shape.
    - rescale is often performed with 
       x' = (x - min(x)) / (max(x) - min(x))
    - another form is to remove the magnitude
      to a proportion of itself using the max
     f0	    f1	    f2		MAX		f0/MAX	f1/MAX	f2/MAX
    1455	854	    54		1455	1.00	0.59	0.04
    2044	1255	134		2044	1.00	0.61	0.07
    90	    38	    8		90		1.00	0.42	0.09
----

----
Principal Component Analysis (PCA)
 - complement to k-means
 - for detecting some structure features
 - how features relate to each other
 - relies on the eigen decomposition of the dataset
   covariance matrix
 - expects features to be centered and on a similar scale
 - as name implies, is intended for reducing redundant features
 - builds on other relational concepts
  - Covariance:
   Cov(X,Y) = average((X-average(X)) * (Y-average(Y)))
   - if x and y move in the same direction together 
     then when x is above its average, y should be above
     its average.
   - has a problem of scale where its not clear 
     what a "high covariance" is without its data
  - Correlation:
   Corr(X,Y) = Cov(X,Y) / (stdDev(X) * stdDev(Y))
    -solves to-
   Corr(X,Y) = average((X-average(X))/stdDev(X) * (Y-average(Y))/stdDev(Y))
    - has property of going between -1.0 to 1.0
   z-score:  
    - rescale X to (X-average(X))/stdDev(X) is 
   - either resulting in zero doesn't mean 
     that no relation exists - it means the 
     relation, if any, is not a matter of moving
     up and down together (a perfect circle solves 
      to 0 but has a very definite relationship)
----   