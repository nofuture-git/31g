----
Notes AI
Artificial Intelligence,A Modern Approach (1st Ed. & 3rd Ed)
Stuart J. Russell and Peter Norvig
Prentice Hall, Englewood Cliffs, New Jersey
ISBN 0-13-103805-2 1995 (1st Edition)
ISBN 978-0-13-604259-4 2010 (3rd Edition)

code examples in Java (20160229)
https://github.com/aimacode/aima-java
----

----
AI is presented as a kind of API
 - in the broader use of the word its 
   a program plus some kind of ability 
   to perceive.
 - has a lot of technical words with 
   specific meanings
 - practical math ports used in AI
  - Hidden Markov Models: used in lang-rec being based 
    on existing math and allowed for training using 
	speech samples.
  - Bayesian network:  deals with probablity and 
    making decisions with uncertainty involved.
 - recent changes: the knowledge base from which 
   to work seems to be as, if not more, important
   to generating solutions than the algorithm used
   - in other words, worst algo with 100M training 
     samples does better than best algo with only 1M
 - entity is the thing which makes choices
   
 - my own observation:
 The problem with the term A.I. is that it is target
 to all the investigations of L.W. - therefore, A.I.
 cannot be defined as a "thing"; rather, it is a catalog
 of computational practices whose effect is something
 we would call "intelligence" in ordinary language.
----

----
Agent
 - is part of an environment
 - has some means of perceiving it
 - has some means of effecting it
 - must have some kind of utility for measuring success.
 - preception is the input, action the output
  - the relation between them is called 'mapping'
 - has autonomy, determines actions based on its
   own exp of perceptions (with some kind of useful 
    starting point)
 - since AI is bound up to our environment it is 
   considered both the program and the senses (e.g. cameras)
 - AI that exist only virtually are named 'software agents'
   or 'softbots' as a portmanteau)
 - the general framework for an agents starts with 
   its precepts, actions, goals and environment
 - text includes example of a taxi driver
    Precepts: Cameras, speedometer, GPS, sonar, mic
    Actions: steer, eccelerate, brake, talk to passenger
    Goals: safe, fast, legal, comfortable trip, profit
    Envrionment: roads, traffic, peds customers
 - precept-action is too simple (a.k.a. reflex-agent), 
   agents will also need to have some kind of 
   register for 'internal-state'
    - internal state requires 
	 (1) how the environment changes independent of the agent
	 (2) how the environment changes dependent on the agent's actions
 - in addition the agent needs some overall kind of vision to choose
   an action (a goal)
 - a more advanced agent will perceive a series of actions from actions
   having many possiable choices and choose the one to maximize goals
  - this becomes further complicated by probability.
----

----
Expressing State (Represent the Environment)
 - text divides expression of state into three modes of 
   increasing complexity
  - atomic: each state is indivisible, a sinlge fact
    like an enum
  - factored: each state is a fixed set of attributes (variables)
  - structured: is factored states which are related to each other
----

----
Properties of an environment
 - accessible (Fully observable) vs. inaccessible (partially observable): 
   the more accessible the less need for an internal state since 
   conditions may be resolved at any time.
 - deterministic vs nondeterministic(stochastic): where future environment state
   is purely a function of the current state plus agent's actions.
 - episodic vs. nonepisodic(sequential): exp is circular or continuous, where circular
   exp only matters, "this time around"
 - static vs. dynamic: where the environment can change while the agent 
   is thinking
 - discrete vs. continuous: when the percepts are clear and limited to 
   a number of moves or not.
 - single agent vs. multiagent: is the environment being effected by
   other agents.
 - known vs. unknown: where the outcome of an action is not even known
   and must be discovered.
   
----
  
----
Problem Solving
 - must first involve a end-goal
 - goal formulation is the first step in problem solving
 - problem formulation is the next step
 - the term 'search' is taking a problem as input and 
   returning a solution (sequence of actions) as output.
   - a successor function x, S(x) is the general form 
     of, given 'x', 'S(x)' is a set of reachable states.
   - 'state space' is a list of all possiable states 
   - goal-test takes a state and determines if it meets 
     a goal.
   - path cost takes into account that more than one sequence 
     achieves the goal but at different costs
   - a problem is then defined as a type having 
    (1) initial state
	(2) operators (successor functions)
	(3) goal-test
	(4) path-cost functions
   - additionaly there is a search-cost where trying to solve
     the problem is costly on its own
 - performing the actions is called execution
 - the general form is then 
    formulate, search, execute
 - agent then finds a new goal
 - a contingency problem is one where the state must be 
   re-eval'ed after each action resulting initialy in a 
   complex desicion tree.
 - exploration problem is when the agent doesn't know the 
   effects of its actions and must experiment to disover them
----
 
----
Search Strategies
 - are judged by the metrix
  (1) Completeness:  is the strategy guaranteed to find a solution
       there is one.
  (2) Time Complexity: how long does it take to find a solution
  (3) Space Complexity: how much memory does it need to perform a search
  (4) Optimality: does the strategy find the highest-quality solution when
       there are several different solutions.

 - there are two major groupings of search strategies
  (1) uninformed search: have no info about the number of steps or the
      path cost from the current state to the goal (aka Blind search). 
  (2) heuristic search (aka informed search): simply are better informed 
      and can resolve quicker
  
 - Branching Factor: the number of new states yielded for resolving any single state
  - 'b', variable for branching factor's max number of successors of any node
  - 'd' variable for the depth of the shallowest goal node
  - 'm' variable for max length of any path in the state space
  - time: measure of the number of nodes generated during a search
  - space: the max number of nodes stored in memory
 
 - there are six minor kinds of uninformed searches (aka blind search)
  (1) Breadth-first search: root node is fully expanded then its direct
      descendents are fully expanded followed by thier descendents and so on.
	 - in general form, nodes at depth 'd' are expanded prior to nodes at depth
       'd + 1'
     - is very expensive: at level one there is 'b', at two there is 'b^2' 
	   at three there is 'b^3'
	 - the max number of nodes expanded is 
	   1 + b + b^2 + b^3 + ... + b^d
	 - at 100 nodes per second with a cost of 100 bytes per node a depth of 14
	   would take 3500 years to resolve and 11111 terabytes of memory
  (2) Uniform cost search: like Breadth-first but always expands the least 
      costly node first
	 - the general form of node expansion cost is 'g(n)'
	 - as such breath-first search expansion cost is 'depth(n)'
	 - will find the cheapest solution if the path cost never decreases
	   'g(SUCCESSOR(n)) > g(n)' for every node
     - the frontier will be an priority queue (no native implementation 
        in .NET @4.6.1)
  (3) Depth-first search: expands a node down to a terminal level
     - upon terminal node goes back up only one and expands sibling
	 - only requires mem for current terminal node and upper, unexpanded 
	   siblings.
	 - general form of memory is branching factor * depth (b*d) as compared
	   to b^d mem for breadth first
  (4) Depth-limited search: avoids the overflow of depth-first by having a 
      cutoff
	  - can be improved by using the state-space's diameter as the cutoff
  (5) Iterative deepening search: is like Depth-limited except that the 
      cutoff is iterated (starting at 0)
	  - nodes at the bottom of the search tree are expanded only once
	  - those just above bottom are expanded twice and so on
	  - the root is expanded then d + 1 times
	   (d + 1)*1 + (d)*b + (d - 1)*b^2 + ... + 3b^(d-2) + 2b^(d-1) + 1b^d
	  - the sum proves that the cost of expanding nodes multiple times 
	    is not as much as expected (for d=10, breadth-first was 111111 and 
		 iterative-deepening is 123456)
		- the more the nodes the less the cost of multiple expansion
  (6) Bidirectional search: search both foward from initial state and backward
      from the goal and two searches meet, the solution is returned
	  - can be very tricky to implement, for example what are the states that 
	     are the predecessors of the checkmate goal in chess.
	  - some kind of manager is needed to know where the meet happens
	  
 - Comparison of uniform search 
          (1)      (2)      (3)              (4)           (5)      (6)
 Time     b^d      b^d      b^max-depth      b^cutoff      b^d      b^(d/2)
 Space    b^d      b^d      b*m              b*cutoff      b*d      b^(d/2)
 Optimal  true     true     false            false         true     true
 Complete true     true     false            true*         true     true
 
 - Avoiding repeated states
  - do not return from the state you just came from
  - manage paths and return only unique ones
  - do not generate any one state more than once
----

----
Constraint Satisfaction Search
 - aka CSP
 - state is a set of variables with values
 - goal-test specifies a set of contrants the values must obey
  - where each value must past some list of propositions
 - in general 
  - each variable (Vi) has a domain of (Di)
   - domain is the set of possiable values a variable may have
   - domains may be continuous or discrete
  - example is the 8-queens problem, where queen 1 is in the first 
    column of the board and queen two is in the second column of 
	the board.
    - the domains for both queen 1 and queen 2 is (1 .. 8)
	- the constraint could be represented as pairs of numbers
	  each representing a possiable attack 
	   - at column 1, row 1 we could attack column 2, row 1 and 
	     column 2, row 2, etc.
	   - solve for allowable pairs as ((1,3),(1,4),(1,5),...(2,4),(2,5),...)
	- any discrete CSP may be reduced to a binary CSP
 - general-purpose algorithm of discrete CSP
  - init of all variables unassigned
  - all ops will set variable values from discrete list
  - goal-test will determine if all variables are assigned 
    and all constraints are satisfied
 - a performance improvement happens here since constraints 
   - for example with the 8-queens, depth-first search will
     blindly continue to expand the nodes at (1,1) - which has
	 obviously already blown it.
 - backtracking search checks whether any constraint has been 
   violated by the variable assignments up to this point and pulls
   back in such a case
   - backtracking has shortcoming of getting to some end, finding 
     a violation and only going back a little when said violation 
	 implies a need to go back much further
 - foward checking will reduce the choices of the yet-to-be assigned
   variables since the current assigned variables imply obvious limits
   on the yet-to-be assigned variables possiable values.
   - this is a special case of "arc consistency"
   - arc consistency exhibits a form of constraint propagation
 - most-constraining-variable: in terms of heuristic the next 
   to receive a value should be the one involved in the largest 
   number of constraints on other yet-to-be-assigned variables
 - least-constraining-value: the value which should be used next 
   in assignment is a value that rules out the smallest number of values
   in variables connected to the current variable by constraints.
----

----
Informed Search Methods
 - these will involve using problem-specific knowledge 
 - when using a general-search the only place to apply 
   problem-specific knowledge is at the queuing function
  - knowledge to make a determination is called the 
    evaluation function
  - an evaluation function typically returns an int 
    which describes desirability
 - when a queue is order by desirability then its called
    best-first search
   - this is misnomer since such a ordered desirability would
     not be a search at all but simply a march
   - in reality eval functions sometimes get it wrong
 - desirability must be an estimate of the cost of the 
   path from a state to the closest goal
   
 - two common best-first are
  (1) Greedy Search: node whose state is judged to be closest to 
      the goal is expanded first.  
	- built on idea the best-first should try to minimize the estimated cost
	  to reach a goal
	- calc of estimated cost is called a heuristic function (denoted by 'h')
	- has requirement of h(n) = 0 means Goal!
	- prefers to take biggest bite possible out of remaining cost to reach 
	  a goal without considering if this particular choice is best in the long run
	 - this is not always the best solution
	 - susceptible to false starts, where choosing most-now can lead to a 
	   dead-end
	 - must check for repeating state or will get stuck going back-and-forth
	   in the same states while looking on at its goal
  (2) A*search: combines the greedy of biggest gain now with uniform-cost, or
      sum of cost up to now.
	  - so we have a possiable 'next node', we add how much the total cost would 
	    be to choose it to the greedy-gain from choosing it (or the amount we 
		reduced to reach our goal [h(n) = 0]) and then choose whichever node has the lowest 
		value.
	   - it works so long as the heuristic function (the op of a greedy search)
	     never over estimates the cost to the goal
	   - never over estimating the cost is called a 'admissible heuristic'
	  - the sum of uniform cost and heuristic cost is denoted as
	    f(n) = g(n) + h(n)
	  - whenever h(n) is an admissible heuristic the f(n) will always 
	    rise on each expanded node - this is called monotonicty
		- given two nodes 'parent-node' and 'child-node' and we calc the f(n)
		  such that 
		  f(parent-node) = g(parent-node) + h(parent-node) = 3 + 4 = 7
		  f(child-node)  = g(child-node) + h(child-node)   = 4 + 2 = 6
          
		 we see that f(child-node) is actually less than f(parent-node) so its
		 not a monotonicy h(n)
	  - we restore monotonicy by branching in such a case and taking the f(n) 
	    of the parent - this is called 'pathmax equation'
      - A* is optimally efficient for any given hueristic function, no other
        algorithm is guaranteed to expand fewer nodes than A*
----		

----
Heuristic Function:
 - has no particular formula for resolution (like how matrix alg walls 
   up at eigen vectors)
 - text uses 8-block puzzle (those cheap plastic puzzles with eight 
    tiles and one empty one)
 - tile in the corner could move 2 places
 - tile in the middle could move 4 places
 - the rest could move 3 places
  - that averages to a branch factor of 3
 - there are 9! possiable arrangements of the tiles
 - cannot move diagonial so calc city distance is called manhattan distance
 - effective branching factor is an algebra eq. solved for with guess of 
   total number of nodes expanded at some depth
  - at depth 5 with 52 nodes we find a solution then effective branching factor
    is solved using the formula from breadth-frist max possiable nodes
	N = 1 + b + b^2 + b^3 + ... + b^d
   52 = 1 + b + b^2 + b^3 + b^4 + b^5 
   - solve for 'b'
   - higher numbered results are always better and are stated
    has 'such-and-such heuristic f(x) dominates the other such-and-such f(x)'
 - still not clear how to derive a heuristic function - for distance it was 
   easy since we know the max straight-line distance and had something to subtract from
 - sometimes using a less restricted operator helps solve for the hueristic function
  - with the block problem tile movement (operator) was limited in direction and if
    the state was empty - if we loosen these restrictions its called 'relaxed problem'
  - when the operator is written down in formal syntax, relaxing it can be automatic
 - another trick is to have a catalog of heuristics and wrap them in a function that 
   returns the max.
 - yet another trick is to only solve for some subproblem within the whole and use 
   that heuristic for the whole.
----

----
Advanced Search 
 - Local Search: different kind of algorithm
  - current node: the base node from which Local Search ops
  - moves about amoung neighbors of current node
 - Optimization problems: "find the best state according 
   to an objective function"
   - objective function: the tautological thing which 
     defines the optimization
 - state-space landscape