Notes Linear Algebra

----
Practical Linear Algebra, 3rd Edition
By: Gerald Farin; Dianne Hansford
Publisher: CRC Press
Pub. Date: August 19, 2013
Web ISBN-13: 978-1-4822-1128-3
Web ISBN-13: 978-1-4665-7959-0
Print ISBN-10: 1-4665-7956-0
Web ISBN-10: 1-4665-7959-5
Print ISBN-13: 978-1-4665-7956-9
Web ISBN-10: 1-4822-1128-9
Web ISBN-13: 978-1-4665-7958-3
Pages in Print Edition: 514
----

----
Practical Graph Mining with R:
By: Nagiza Samatova; William Hendrix; John Jenkins; Kanchana Padmanabhan; Arpan Chakraborty
Publisher: Chapman and Hall/CRC
Pub. Date: July 15, 2013
Print ISBN-13: 978-1-4398-6084-7
Pages in Print Edition: 495
----

----
https://www.math.uh.edu/~jmorgan/Math6397/day13/LinearAlgebraR-Handout.pdf
----

----
R in Action, Second Edition
Data analysis and graphics with R
Robert I. Kabacoff
May 2015 ISBN 9781617291388 608 pages printed in black & white
----

----
Regression Analysis with R
By: Giuseppe Ciaburro
Publisher: Packt Publishing
Pub. Date: January 31, 2018
Print ISBN-13: 978-1-78862-730-6
Web ISBN-13: 978-1-78862-270-7
Pages in Print Edition: 422
----

----
https://www.ncbi.nlm.nih.gov/books/NBK253312/
----

----
R Summary
 - is a GNU port of the "S" language where S was a stat lang
   developed at Bell Labs by John Chambers.
 - is open-source, dev'ed by Robert Gentlement and Ross Ihaka
 
 -package management
  - get more libs at https://cran.r-project.org/web/packages/available_packages_by_name.html
  - take the zip (window) and unzip to R install dir named \library
  - load using 
  library(sna)
 
 - test the version at runtime
  R.Version()
  
 - package version
  packageVersion("packageName")
  
 - package mangement
  - worked within VS 2017 R Interactive
  install.packages("packageName", lib="C:/Program Files/Microsoft/R Client/R_SERVER/library")
----

----
R Ops
 - Assignment
  <- assignment and scoped globally
  =  assignment with local socpe
  -> assignment in reverse where variable is right operand
 
 - Logical 
  TRUE 
  or just 'T'
  FALSE
  or just 'F'
  < 	less than
  <=  	less than or equal to
  > 	greater than
  >=	greater than or equal to
  ==	exactly equal to
  !=	not equal to
  !x	Not x
  x | y	x OR y
  x & y	x AND y 
  isTRUE(x)	test if X is TRUE
  isTRUE(2 > 1 | 1/0) #is short-circuit logical ops
  
 -Mathematical Ops
  +, -, *, /  (arithmatic)
  ** or ^     (exponent) 
  %%          (modulus)
  %*%         (matrix multiplication)
  ~           (a kind of R specific thing, see help)
  
 -Flow Control Ops
  if(TRUE) x <- 1 else x <- 2 #inline if
  for(i in c(1:10)) print(i)
  a <- 0; while(a < 10) a <- a+1
  apply(x,(1|2),cos) #where, x is a matrix
                   # 1 is for each row, 2 for each column
                   # cos is the builtin f(x) cosine
  lapply(x,cos) #apply cosine builtin to every element
  
 - Casting
  as.data.frame(x)
  as.matrix(x)
  as.vector(x)
  as.list(x)
----

----
R Builtins
 #define a vector
 a <- c(1,3,2)
 b <- c(2,8,9)
 bb <- seq(start,end[,step]) #gen' stepped sequences 
 cc <- c(1:10) gen' single sequence
 dd <- b[!b %in% 9] #a kind of selector to get the vector less this item
 
 #assignment can work both ways
 c(1,3,2) -> a  
 c(2,8,9) -> b
 
 #multiple statement on single line are ; sep'ed
 c(1,3,2) -> a; b <- c(2,8,9)
 
 #vector index op
 a <- c(1,2,3)
 a[1] 
 a[-2] #means everything except index 2
 
 #transpose 
 t(a)
 
 #scalar multiplication
 7*a
 
 #sum of vectors
 a+b
 
 #inner product
 sum(a*b)
 
 #vector length (e.g |a|)
 sqrt(sum(a*a))
 
 #repeat 0,1 vector
 rep(0,5)
 rep(1,5)
 
 #orthogonal vector have inner product of zero
 v = c(0,5)
 w = c(3,0)
 sum(v*w)  #equals zero
 
 #define a matrix
 #df are read column-by-column 
 A <- matrix(c(1,3,2,2,8,9), ncol = 3)
 #set to read row-by-row
 B <- matrix(c(5, 8, 3, 4, 2, 7), ncol = 3, byrow = T)
 
 #matrix index op
 A[1] #same as A[1,1] 
 A[1,] #row 1
 A[,1] #column 1
 
 #scalar multiplication
 7*A
 
 #transpose matrix
 t(A)
 
 #matrix addition
 A + B
 
 #column-vector multiplication
 b <- c(5,8)
 A %*% a
 
 #matrix multiplication
 #A as r X c and B as c X t, AB is r X t matrix
 A <- matrix(c(1,3,2,2,8,9), ncol=2)
 B <- matrix(c(5,8,4,2), ncol = 2)
 A %*% B
 
 #identity matrix
 diag(1,3)
 
 #find inverse
 A <- matrix(c(1,3,2,4), ncol=2, byrow=T)
 B <- solve(A)
 A %*% B       #identity matrix
 
 #find determinent
 A <- matrix(c(1,0,5,1), ncol=2)
 B <- det(A)
 
 #find eigen values, vectors
 # - the result is a type with two props
 # - ev$val for eigen values
 # - ev$vec for eigen vectors
 ev <- eigen(A)
 
 #IO
 myData <- read.table("C:/MyR/MyRData/MyData.tsv",header=TRUE)
 sink("C:/MyR/MyOutput/output.txt") #will route all output to file
 sink() #sets output back to console.
 MyCsvData=read.csv ("C://SomeFolder//SomeFile.csv", header=TRUE, sep=";")
 plot(MyCsvData$SomeColumn,MyCsvData$SomeOtherColumn)
 
 #requires the package readxl
 SomeMsExcelData <- read_excel("MyExcel.xlsx")
 
 #set the working directory
 setwd("C://Projects")
 
 #namespace conflicts can be handled using full qualified 
 sgd::sgd
 
 #user defined function (in-line)
 myNormalForm <- function(x1, x2) -2*x1 + 4*x2 - 4
 
 #user defined function(multi-line)
 # - written in a separate UTF8 file
 # - then imported into console 
 #  - using 'source("path-to-file")'
 #  - path-separator is unix-style /
 
 vLen <- function(a) {
    sqrt(sum(a*a))
 }
 source("C:/MyR/UserFunctions/LinearAlgebra.r")
 vLen(c(5,4)) #6.403124
 
 #other usefull builtin
 ls()  # display all the variables defined so far in the console.
 rm(a) # gc a variable 
 er<-runif(10,min=-1,max=1) #get 10 random values between -1 and1

 help("[[") #gets help on builtin symbols (displays in browser)
 
 # Numeric builtins
 length(a); mean(a); min(a); max(a); var(a)
 abs(x); sqrt(x); ceiling(x); floor(x)
 trunc(x); round(x[, n]); signif(x[, n])
 cos(x); sin(x); tan(x); #use radians not degees
 log(x); log10(x); log2(x)
     
 #Stat builtins
 cov(SomeData$FieldOne, SomeData$FieldTwo) #covariance
 cor(SomeData$FieldOne, SomeData$FieldTwo) #correlation coefficient
 sd(SomeData$FieldOne) #standard dev
 
 #dynamic types
 myDyn <- list(name="myType", someProp=11, anotherProp=c(1:10))
 #access props with '$' instead of c-style dot
 myDyn$name
 #can also use an index
 myDyn[1]
 
 #data frames are dynamic runtime tables
 a <- c(1, 2, 3)
 b <- matrix(c(4:15), nrow=3, ncol=4)
 p <- c("R","is","easy")
 df <- data.frame(a,b,p)
 
 #get runtime type info
 str(someDataFrame)
 
 #object property reflection
 names(SomeObjectInstance)
 
 #dataframe with property-style names
 prop00 <- 11.0
 prop01 <- TRUE
 prop02 <- "a string"
 myDf <- data.frame(prop00, prop01, prop02)
 
 #dataframe indexing
 myDf[1]       #would get first column, all rows
 myDf[1,]      #get the first row, all columns
 myDf[,1]      #get the first column, all rows, as single array
 myDf[1:5,]    #get first 5 rows, all columns
 myDf[,1:2]    #get first 2 columns, all rows
 myDf[1:2]     #same as the last
 myDf[1:5,1:2] #get first 5 rows first 2 columns
----

----
R Graphix
 x <- c(1,3,4,5,8)
 y <- c(2,4,7,3,9)
 plot(x,y) #basic, no labels
 plot(x,y, type="l", col="black", xlab="My X-axis label", 
           ylab="My Y-axis label", main="My Header label",
           font.main=2, font.lab=1,pch=19)
 par(mfrow=c(2,2)) #to show multiple graphs in the R Plot window
----

----
Points & Vectors
 - most basic 2D a point is two coords
 - three or more points are collinear when 
   they all lie on a single line
 - two points are collinear since it takes 
   two points to form a line.
 p = [p1,p2]
 q = p + v
 [q1,q2] = [p1,p2] + [v1,v2] = [p1 + v1, p2 + v2]
 v = q - p
 [v1,v2] = [q1,q2] - [p1,p2] = [q1 - p1, q2 - p2]
 
 - Vector Space: consists of a set of vectors 
   and the operations of scaling and addition.
   - scaling op is multplicative with operands
     as a scalar and a vector.
	- the product is then a scaled vector and
	  a likewise member of the same vector space
    - each component (i.e. item) in the vector
	  is scaled by some multiple
   - addition op is performed over two operand
     vectors 
 
 - vector is a difference of two points
   which describes a direction and distance
  - the components of v is v1 and v2 which indicate
    a rise & run from point p
  - two vectors are equal if they have the same
    component values just as two points are equal
	if they have the same coords
  - a vectors components are called a 'tail' and 'head'
  
 - points and vectors are different geometric entities
  - distinguishing them it to achieve geometric constructs
    that are coordinate-independent.
  - coordinate-independent constructs are manipulations 
    applied to geometric objects that produce the same 
    result regardless of the location of the coordinate 
    origin.
  
 - in practice:
  - subtracting to points yields a vector
  - adding or subtracting two vectors yields another
    vector
  - multiplying by a scalar is called scaling.
    The result s*v adjusts the length by the scaling
	factor
	- the direction is unchanged if s is greater-than 0
	- the direction is reversed if s is less-than 0
  - adding a vector to a point yields another point

  - a 'well-defined' operation is one which is coordinate 
    independent
  - scaling a point (s*p) is not well-defined
  - scaling a vector (s*v) is well-defined
  - adding two points (p+q) is not well-defined since
    it is dependent on the coordinate origin
  - a vector-field is one where each point is 
    assigned to a vector 
  - moving a continuous field into a vector-field
    is called discretizing - taking some sample of
	points and assigning each a vector
	
 - a Matrix is a map of vectors to vectors
 
 - Tensor order: how a value is indexed in a vector
   e.g. v[1]
   "tensors of unity order", the nature of indexing 
   a vector with a single index
   - scalars may be viewed as zero-order tensors
   - matrices are second-order tensors
   
 - Hyperplane: a kind of cross-section of some
    set of dimensions, being one-less dimension than
    its container.
   - like the classic three dim graph from econ where
     we hold one dimension constant
----

----
Graph Theory (Formal)
 - beyond common use, is a formal abstract concept
 - is composed of points (also called vertices)
 - has lines (also called edges) which connect points
 - represent structured data
 
 Terms
 - vertices (nodes): is the discrete data of the graph 
 - edges (links): represent the relation amoung the vertices'
 - join: formal name for what the edge is doing; namely, joining
    two vertices.
 - adjacent: the state between to vertices when there is one edge
    between them.
 - connected: when a indirect route connects one vertex to another
 - distance: the number of edge traversals required to reach one
    node from another
 - geodesic: fancy name for shortest-distance
 - loop: the state of an edge connecting a vertex back to itself
 - multiple edge: when there is another edge which joins the same
    pair of vertices
 - simple graph: a graph with no loops or multiple edges
 - degree: the number of edges being joined to a single vertex
 - density: the number of edges over the number of possiable edges
 - connectedness: fraction of all groups of two nodes where there is an 
    edge between them
 - betweenness: the number of times some node was passed-through 
    for every possiable node-pair.
 - subgraph: a subset of vertices and edges of another graph
 - induced subgraph: a subgraph where only the vertices or only 
    the edges are known.
 - bijection: ("by Jove its a perfect match") one and 
      only one girl for every boy and nobody is alone.
 - surjective: ("sure we can share") one girl 
      for every boy but some boys have to 
      share the same girl, but at least 
      no one is alone
 - injective: ("intense feelings of loneness") one girl 
      for every boy and no boy ever shares the same 
      girl, but some girls are all alone
 - non-surjective & non-injective: ("no! no!") state of 
      some girls being all alone and some girls sharing boys
 - isomorphic graph: when two graphs are drawn the same
 - automorphic graph: when two graphs are drawn the same
    and their underlying structured data is the same.
 - label: by being labeled a vertex or edge becomes definititive 
    like being an instance.
 - directed graph (digraph): when the edges are an ordered pair
 - edge head: the v1 in the ordered pair (v1,v2)
 - edge tail: the v2 in the ordered pair (v1,v2)
 - indegree: number of head-edges for a given vertex
 - outdegree: number of tail-edges for a given vertex
 - clique (egocentric): set of connected vertices where beginning at 
    one vertex you may navigate to any other member of the
	clique via one edge
 - complete graph: every vertex is connected to every other 
    vetex via an edge and is equal to n*(n-1)/2
 - path: another name for a route, also called a walk
 - simple path: if the route never visits the same vertex twice
 - open path: when the first and last vertices are different
 - closed path: when the first and last vertices are the same
    also known as a cycle
 - simple cycle: same as simple closed path except the origin 
    vertex is the only one visited twice (being the terminus)
 - tree graph: has an originating root vertex
 - parent\child: same as used in XML
 - leaf: a child vertex with not children of its own
 - descendant: in tree graph, used the same as in a family tree
 - ancestor: again, tree graph, like a family tree
 - weighted graph: where the edge is the vertex pair and a weight
 - hub: a vertex having a high outdegree
 - authority: a vertex having a high indegree
 - morphism: specific to directed graphs being another general name
    for an edge but differs by having some edges labeled and then 
    expressing others edges as label-of-labels or combinations of 
    other edge labels 
    (in DOT syntax where the '[label="blah"]' is how to label an edge)
    X -> Y [label="f"]
    Y -> Z [label="g"]
    X -> Z [label="g & f"] //named in terms of the other two

----

----
Representing a Graph
 - adjacency list: represent each vertex as a key and the value 
    as an array of adjacent vetices.
 [1], [2,3,3]	would mean vertex 1 has one edge connecting it 
                 to vertex 2 and two edges connecting it to vertex 
 [2], [1,4]     vertex 2 has one edge connecting it to vertex 1 and 
                 one edge connecting it to vertex 4
 
 - adjacency matrix: an exhaustive list of propositions concerning 
    is vertex v_i connected to vertex v_j 
   - requires alot more memory to represent; namely n^2

	.-         -.
    | 0 1 1 0 0 |  would mean the same for the first two rows as 
	| 1 0 0 1 0 |   above
	| 1 0 0 1 0 |
	| 0 1 1 0 1 |
	| 0 0 0 1 1 |
	`-         -'
  - use the 'sna' R package for application of many graph terms
    to actual instance of adjacency matrix
  myAdjGraph <- rgraph(n,m=1,tprob=0.5,mode="(digraph|graph)" [...]) #sna adjancency graph
   - n is number of vertices, matrix is always square
   - tprop is the probability of a link
   - mode is obvious, other default value args see 'help(package="sna")'
  myIGraph <- graph.adjacency(myAdjGraph, mode="(undirected|directed") #type cast to igraph
  - applied terms 
   plot(graph)               #view it as a pic
   degree(myIGraph)          #degree
   gden(graph,mode="graph")  #density 
   connectedness(graph_adj)  #connectedness
   is.connected(graph)       #test if connectedness equal to 1
   is.isolate(graph,[1-9]+)  #test if node, by index, has no edges
   betweenness(graph)        #betweenness
   geodist(graph_adj)        #shorted-dist, calc'ed on whole
   ego.extract(graph_adj)    #get all the cliques
 
 - import adjacency matrix example
  library(sna)
  library(igraph)
  myData <- read.table("C:/Temp/MyData.tsv",header=FALSE)
  myMatrix <- as.matrix(myData)
  myAdj <- graph.adjacency(myMatrix, mode="directed")
----

----
Data linkage (theory)
 - links are est. on a record-by-record basis
 - two accuracy errors 
  - true non-match classified as match
  - true match classified as non-match
 - metrix of methods
  - divide into four groups 
    Classified-as | True 
       Match      | Match
   A     0            0
   B     0            1
   C     1            0
   D	 1            1
   - Sensitivity = A / (A + B); ability to classify matches
   - Specificity = C / (C + D); likewise for non-matches
   - Positive Predictive Value: A / (A + C); ratio of classified matches
                                             to actual matches
   - Negative Predictive Value: D / (B + D); likewise for non-matches
   
 - starts with comparable records
  - data should be atomic broken down into smallest possiable parts
  - columns with high covariance should have one of the two removed
  
 - methods 
  - deterministic: the more straight-foward method
   - takes itself two forms
    - single-step: exact equality char-for-char by ordinal
	- iterative: a set of rules taken in steps
	 - SEER-Medicare linkage algo (each field may be a
	    hashed value instead of the literal, still works):
	  1.0 match on SSN AND
	   1.1 match on Fname & Lname OR
	   1.2 match on Lname, month-of-dob and gender OR
	   1.3 match on Fname, month-of-dob and gender
      2.0 match on Lname, Fname, month-of-dob, gender AND
	   2.1 8 of 9 SSN digits match OR
	   2.1 any two of 
	    2.1.1 year-of-dob
		2.1.2 day-of-dob
		2.1.3 middle initial
		2.1.4 dod (date of death)
		
  - probabilistic (aka stochastic, fuzzy): assumes certian 
     fields\attributes\column\properties have more discriminatory
	 power than others.
	- based on Fellegi & Sunter  "A Theory For Record Linkage"
	- links designated as match, possiable match, non-match
	- comparison space: the cartesian product to the two
	  record sets
	 - each pair in the comparison space is either a 
	   true match or true non-match
	- blocking: idea that cartesian product is too big
	   so reduce fields to most useful or those which
       are not null (subset).
	- m-probability: probability that true matches agree 
	   on a field
	 - calc'ed by having some informed data where the link
	    is already est. (thing informed machine learning)
		then get the prob. of each field actually being equal
	- u-probability: probability that false matches randomly 
	   agree on a field
	 - partial agreement weights: calc'ed in similar manner 
	    where you have est. the link already and for definite 
		non-matches fields values are equal.
    - agreement weight: a per-field weight calc'ed as
        log2(m/u)
	- disagreement weight: per-field weight calc'ed as
        log2(1-m/1-u)
	- each field is then determined as match\non-match 
	  and the result is a sum of each fields 
	  agreement\disagreement weight.
	- determining if an individual field is a match 
	   still remains but could be enhanced using the 
	   Jaro-Winkler String Distance algo.
	- some threshold is required still to say that 
	  such-and-such weight means "IsMatch"
	- calc threshold for "IsMatch" using
	   E: (guess) the number of expected matches between datasets
	   P: the desired probability that a record is a match
	   A: number of records in first dataset
	   B: number of records in second dataset
	   thres <- log2(P/(1-P)) - log2(E/((A*B)-E))
	  
  - Naive Bayes: this typical AI method could be used
    when there exist some training data - would probably
	look like:
	  S(f1) W(f1) S(f2) W(f2) S(f3) W(f3) ... S(fi) W(fi) IsMatch
	where S(f1) is field 1 from the first dataset and W(f1) is 
	field 1 from the second data set.
----

----
Vector Length
 - may represent distance, velocity or acceleration
 - length of a vector is called magnitude
 - since a vector's components are the rise & run
   the vector's length may be assigned as the 
   hypotenuse of a right triangle
 - this is named the Euclidean norm  
   |V|^2 = v1^2 + v2^2
   |V| = sqrt(v1^2 + v2^2)
 - a normalized vector has a unit length of one
   |W| = 1
  - normalized vectors are also known as unit vectors
  - not every vector is a normalized one
 - to normalize a vector is divide each component by 
   Euclidean norm
   W = v/|V|
   
   - example, not a normalized vector
   v = [3,7]
  |V| = sqrt(3^2 + 7^2)
  |V| = 7.615773106
   W = [3/7.615773106, 7/7.615773106]
   W = [0.3939192986, 0.91914503]
   
  - example, a normalized vector
   v = [5,0]
  |V| = sqrt(5^2,0^2)
  |V| = 5
   W = [5/5, 0/5]
   W = [1, 0]
   
 - the distance between two points is the
   Euclidean norm of the vector of the two points 
   q = [-1,2]
   p = [1,0]
  q-p  = [-1-1, 2-0] = [-2,2]
 |q-p| = sqrt(-2^2 + 2^2)
 |q-p| = 2.828427125
----

----
Combining Points
 - although not well-defined there is a way to 
   combine two points
  -given two points p and q, v = q-p, t is a scaler
 r = p + t*v
 r = p + t*(q-p) 
 r = p + tq - tp
 r = p - tp + tq
 r = (1-t)*p + tq
      t = |r-p| / (|r-p| + |q-r|) 
  (1-t) = |q-r| / (|r-p| + |q-r|)
  
 - example
  p = [2,4], r = [6.5,7] q = [8,8]
  r = (1-t)*p + tq
  [6.5,7] = (1-t)*[2,4] + t*[8,8]
  
  |r-p| = |[6.5 - 2, 7 - 4]|
  |r-p| = |[4.5, 3]|
  |r-p| = sqrt(4.5^2 + 3^2)
  |r-p| = 5.408326913
  
  |q-r| = |[8 - 6.5, 8 - 7]|
  |q-r| = |[1.5, 1]|
  |q-r| = sqrt(1.5^2 + 1^2)
  |q-r| = 1.802775638
  
  |r-p| + |q-r| = 5.408326913 + 1.802775638
  |r-p| + |q-r| = 7.211102551
  
  t = |r-p| / (|r-p| + |q-r|) 
  t = 5.408326913 / 7.211102551
  t = 0.75
  
 (1-t) = |q-r| / (|r-p| + |q-r|) 
 (1-t) = 1.802775638 / 7.211102551
 (1-t) = 0.25
----

----
Dot Product
 - concerns the two vectors 
  - are they the same vector
  - are they perpendicular 
  - what angle to they form
 
 |v - w|^2 = |v|^2 + |w|^2
 (v1 - w1)^2 + (v2 - w2)^2 = (v1^2 + v2^2) + (w1^2 + w2^2)
  -binomial expansion
 (v1^2 - 2*v1*w1 + w1^2)+(v2^2 - 2*v2*w2 +w2^2) = (v1^2 + v2^2) + (w1^2 + w2^2)
 (v1^2 - 2*v1*w1 + w1^2)+(v2^2 - 2*v2*w2 +w2^2) - (v1^2 + v2^2) - (w1^2 + w2^2) = 0
  v1^2 - 2*v1*w1 + w1^2 + v2^2 - 2*v2*w2 +w2^2  + -v1^2 +  -v2^2 + -w1^2 + -w2^2 = 0
   (v1*w1) + (v2*w2) = 0
      v * w = 0
 
 - is then used to define the cosine of two vectors
        w   
       7
      /.
     / .
    /  .
   /   .
  /    .
 /)____.___> v
 -solve for the angle by
  cosθ = v*w / |v|*|w|
----

----
Orthogonal projection
 - the orthogonal project of w onto v is defined as
 
  u = ((v * w)/|v|^2) * v
 - using this the vector w can be decomposed into 
   two perpendicular vectors
  
  u┴ = w - ((v * w)/|v|^2) * v
----

----
Other vector rules
 - Cauchy-Schwartz inequality
 - used for the study of more genearl vector spaces
 (v * w)^2 <= |v|^2 * |w|^2
 
 - triangle inequality
 |v + w| <= |v| + |w|
 
 - barycentric coordinates 
  - interpreting vector v as the diff 
    between points q and p 
 v = q - p	
----

----
Linear Combination
 - two vectors are parallel they are called
    linearly dependent
 - not being the case is called
    linearly independent
 
 - linear combination is taking two 
   linearly independent vectors to make 
   a new vector
 u = r*v + s*w
 
 a1 <- c(2,1)
 a2 <- c(-2,4)
 v <- c(0.5,1)
  
 myLinearCombo <- v[1] * a1 + v[2] * a2
 
 - this is easier in using a matrix
 A <- matrix(c(2,1,-2,4), ncol = 2)
 myLinearCombo <- A %*% v
----

----
Lines defined
 - different ways to specify a line geometrically
  - two points on a single line
  - one point and one vector on a line
  - one point and one vector perpendicular to the line
 - likewise, there are different was to specify a line
   mathematically
  - parametric
  - implicit
  - explicit
  
 - Parametric equation of a line
  - where p ϵ E² and v ϵ R² (whatever that means)
  l(t) = p + tv  
   v = q - p (barycentric coords)
  l(t) = (1 - t)p + tq  (this form is aka linear interpolation)
  - can be used to find points on a line given two points
  
 - Implicit equation of a line
  a*x1 + b*x2 + c = 0
   a = a1
   b = a2
   c = a1*p1 - a2*p2
   
  - solve for a
  p <- t(c(2,2)); q <- t(c(6,4))
  v <- q - p
  # a = [-v2, v1]
  
  a <- t(c(-1*v[1,2], v[1,1]))
  #a = [-2,4]
   
  - solve for c
  c <- -1 * a[1,1] * p[1,1] - a[1,2] * p[1,2]
  #c = -4
   
  - implicit form is
   implicitForm <- function(x1,x2) -2*x1 + 4*x2 - 4
  - this can be used to determine if a point is on a line
   implicitForm(2,2) #0
  - to test equality this should because of floating point errors
  #d = (a1*x1 + a2*x2 + c) / |a|
  isOnLine <- function(x1,x2) (normalForm(x1, x2)) / vLen(a)
  
 - Explicit equation of a line
  x2 = ~a * x1 + ~b
  - where  ~a is -a/b and ~b = -c/b
  - this the classic form with intercept and slope
  - ~a is the slope (rise/run) 
   - slope is formally defined as tan(θ)
   - problem with explicit form is flat lines
     have and infinite slope
----
   
----
Line Equation conversion
 - Parametric to Implicit
 l: l(t) = p + tv        parametric
 l: a*x1 + bx2 + c = 0   implicit
   a = [-v2, v1]
   c = -(a1*p1 + a2*p2)
   
 - Implicit to Parametric 
 l: a*x1 + bx2 + c = 0   implicit
 l: l(t) = p + tv        parametric
   v = [b, -a]
   p = [-c/a,0] -or- [0,-c/b]
      choose by whichever is greatest
	   abs(a) > abs(b), choose [-c/a,0]
	   abs(a) < abs(b), choose [0,-c/b]
----

----
Distance of a Point to a Line
 - given any point (r) and line (l)
   how far is r from l
   
 d = (a * (r - p)) / |a|
 
 - implicit eq form
  l: 4 * x1 + 2 * x2 - 8
  r: [5,3]
  
  l <- c(4,2)
  c <- -8
  r <- c(5,3)
  
  d = (sum(l * r) + c) / vLen(l)
----

----
Foot of a given point
 - given some line (l) and some point (r)
   which point on (l) is closest to (r) 
   known as (q)

  t = (v * (r - p)) / (vLen(v))^2
  q = p + t * v
  
  - parameteric eq form
   l(t) = p + t * v
    l: = [0,1] + t * [0,2]
    r: [3,4]
   
  p <- c(0,1)
  v <- c(0,2)
  r <- c(3,4)
  
  t <- (v * (r -p)) / vLen(v)^2
  q <- p + t * v
----

----
Line intersect points
 - calc form for:
  - parametric & implicit
  - both implicit
  - both parametric
----

----
Parametric & Implicit Solve for intersect 
 line_1: l(t) = p + t * v
 line_2: a*x1 + b*x2 + c = 0
 
 - find the parameter (t) with respect 
   to line_1
 - plug-in line1 into line2
  a * [p1 + t * v1] + b * [p2 + t * v2] + c = 0
 - solve for t
 
  t = (-c - a * p1 - b* p2) / (a * v1 + b * v2)
 - if the denominator is zero the two lines are 
   parallel and never intersect
 - check for parallel before solving using 
   by calc cosine of a * v
  cosθ = a*v / vLen(a) * vLen(v)
   - computational tolerance around cos(0.1) 
     and cos(0.5)
 - furthermore, these may be the same lines 
  d = (a*p1 + b*p2 + c) / vLen(a)
   - are the same line if (d) equals zero
 
  -example
   line_1: l(t) = [0,3] + t * [-2,-1]
   line_2: 2*x1 + x2 - 8 = 0
 
  p <- c(0,3)
  v <- c(-2,-1)
  ab <- c(2,1)
  c <- -8
  
  myDenom <- sum(ab * v) #no eq zero
  t <- (-1*c - sum(ab * p)) / myDemon
  
  #use t to solve line_1
  myIntersect <- p + t * v # [2,4]
----

----
Both Parametric Solve for intersect 
 line_1: l1(t) = p + t * v
 line_2: l2(t) = q + s * w
 
 - need to solve so that
  p + t * v = q + s * w
  - rewritten as
   t*v - s*w = q - p
  
 - example:
  line_1: l1(t) = [0,3] + t * [-2,-1]
  line_2: l2(t) = [4,0] + s * [-1, 2]
  
  p <- c(0,3)
  v <- c(-2,-1)
  q <- c(4,0)
  w <- c(-1,2)
 
  #the text gives no explaination for 
  # how we arrive at these values
  t <- -1
  s <- 2
----

----
Both Implicit Solve for intersect 
 line_1: a0*x1 + b0*x2 + c0 = 0
 line_2: a1*x1 + b1*x2 + c1 = 0
 
 - example
 line_1: 1 * x1 - 2 * x2 + 6 = 0
 line_2: 2 * x1 + 1 * x2 - 8 = 0
 
  #the text gives no explaination for 
  # how we arrive at these values
  - intersect x = [2,4]
----

----
Convert degrees to radians
 - R only works in radians, not degrees
 - Circumference = 2*pi*r 
    where r is radius
 - want to work as r = 1
   C = 2 *pi, which is 360 degrees
 - so 180 degrees is just pi
 - conversion is thus where (a)
   is degrees
   
   (a / 1) * (pi / 180)
----

----
Matrix Laws & Names
 - not commutative multiplication
   AB != BA
   
 - associative multiplication
  A*(B*C) = (A*B)*C
  (a*b)*C = a*(b*C)
  a*(B*C) = (a*B)*C = B*(a*C)
  
 - distributive law
  A*(B + C) = A*B + A*C
  (B + C)*A = B*A + C*A
  A*v + B*v = (A + B)*v
  [A + B]ᵀ = Aᵀ + Bᵀ
  A*(u + v) = A*u + A*v
  
 - commutative addition
  A + B = B + A
  
 - associative addition
  A + (B + C) = (A + B) + C
  
 - symmetric matrix is 
   A = Aᵀ
 - dyadic matrix
   A*Aᵀ
 - idempotent matrix
   A = A*A
 
 - other multiplication rules
  (A*B)ᵀ = Bᵀ*Aᵀ
  det(A * B) = det(A) * det(B)
 
 - exponent rules
   A^(r+s) = A^r * A^s
   A^(r*s) = (A^r)^s
   A^0 = I

 - inversion rules
  A^-1*A = I
  A*A^-1 = I
  I^-1 = I
 (A^-1)^-1 = A
 (A^-1)ᵀ = (Aᵀ)^-1
   
----

----   
Reflection 
 - is like flipping & rotating on an axis
  -example
   myR <- matrix(c(1,0,0,-1), ncol=2)
   v <- c(2,4)
   myR %*% v # [2,-4], flipped along x-axis 
           # when c(2,4) is thought of as 
	       # x = 2, y = 4
   myR <- matrix(c(0,1,1,0), ncol=2)
   myR %*% v # [4,2], x & y swapped places
   
   myR <- matrix(c(-1,0,0,-1), ncol=2)
   myR %*% v # [-2,-4] 180 rotation
   
 - rotation matrix is given as where (a)
   is the desired angle of rotation
  
  R = [ [cos(a), -sin(a)],
        [sin(a), cos(a) ] ]
  
  a <- degrees2Radians(45)
  myR <- matrix(c(cos(a),sin(a),-1*sin(a),cos(a)),ncol=2)
----

----
Shears
 - tilt like how italic fonts look to regular
 
 v' = [ [   1,   0],  * [v1,  = [v1,
        [-v2/v1, 1] ]    v2]     0]
		
 - example
 v <- c(2,5)
 vMatrix <- matrix(c(1,(-1*v[2]/v[1]),0,1), ncol=2)
 vTick = vMatrix %*% v # 2,0
----

----
Projections
 - like sunlight casting shadows
 - parallel projections are having all vectors
   projected in a parallel direction
 - when the angle of incidence is 90 degrees
   then its an orthogonal projection
 - otherwise known as an oblique projection
 
 - example
  # a 45 degree rotation
  ui <- 1/sqrt(2)
  u <- matrix(c(ui, ui),ncol=1)
  #defines projection matrix
  A <- u %*% t(u)
  
  #projection of v by 45 degrees
  v <- c(1,2)
  vTick <- A %*% v #1.5, 1.5
----

----
Linear Maps and Area
 - getting the area given two vectors
  a1 = [2,4]
  a2 = [6,3]
  
   |____________(a1)_________________________
 4-|             7_______            T2      |
   |            /        ---______           |
   |           /                  -----______|
 3-|  T3      /                           __-7(a2)
   |        /                          _--   |
   |       /         T            __---      |
 2-|      /                    _--           |
   |     /                __---              |
   |   /              _---                   |
 1-|  /          __---                       |
   | /    ____---                T1          |
   |/__---                                   |
 0-|-------------------------------------------
   |      |      |      |      |      |      | 
   0      1      2      3      4      5      6 
 
  - it is possiable to calc the area of each
    triange (T, T1, T2, T3)
   T1 = 1/2*a[1,1]*a[2,1]
   T2 = 1/2*(a[1,1] - a[1,2])*(a[2,2]-a[2,1])
   T3 = 1/2*a[1,2]*a[2,2]
   
   T = a[1,1] *a[2,2] - T1 - T2 - T3
  
  - determinant is given a 2*T
----

----
Linear System (2X2)
 - defining two linear eq's in 
   matrix format
 - Linear Map is formally defined as
    v' = A*v 
   - in that it preserves linear combinations of
     vectors.   
	
  2*u1 + 4*u2 = 4
    u1 + 6*u2 = 4
	-as-
  [ 2 4   [u1    [4
    1,6 ]* u2] =  4]
 
  - the possiable solutions is named
    solution space
  - roughly divided into 
   (1) exactly on solution vector (u)
       when det(A) != 0
   (2) there is no solution whatsoever
   (3) there are an infinite many solutions
----

----
Cramer's rule
 - based on the geometry of the parallelograms
   formed by a linear system
   a <- matrix(c(2,1,4,6),ncol=2)
   b <- matrix(c(4,4), ncol=1)
   
   u1Numer <- matrix(c(b[1,1],b[2,1],a[1,2],a[2,2]), ncol=2)
   u2Numer <- matrix(c(a[1,1],a[2,1],b[1,1],b[2,1]), ncol=2)
   uDemon <- matrix(c(a[1,1],a[2,1],a[1,2],a[2,2]), ncol=2)
   
   u1 <- det(u1Numer) / det(uDemon)
   u2 <- det(u2Numer) / det(uDemon)
----

----
Gauss Elimination
 - involves tech terms of 
 - back substitution: where we solve for 
   u2 then use its solution-value to solve for u1
  u1 = b2/a2,2
  u1 = 1/a1,1 * (b1 - u2*a1,2)
   
 - foward elimination: the process of getting a[2,1] to equal 0
    which is required for back-sub to work.
 - gauss elimination: the process of foward-elim. followed by
   back sub.
  - based on rule that linear maps do not change linear combos.
    which means applying a linear map to all vectors will not 
	change u1, u2.
 
 - get a[2,1] to zero can be done using a Shear, defined as
  - re-write into column form so Algebraically looks like
               [a[1,1]        [a[1,2]             [b[1,1]
   shearA*(u1 * a[2,1]] + u2 * a[2,2]] ) = shearA* b[2,1]]
   
   a <- matrix(c(2,1,4,6),ncol=2)
   
   #break matrix a into two columns
   aCol1 <- matrix(c(a[1,1],a[2,1]), ncol=1)
   aCol2 <- matrix(c(a[1,2],a[2,2]), ncol=1)
   b <- matrix(c(4,4), ncol=1)
   
   #calc shear
   shearA <- matrix(c(1,(-1*a[2,1]/a[1,1]),0,1), ncol=2)
   
   #take shear time each column
   newACol1 <- shearA %*% aCol1
   newACol2 <- shearA %*% aCol2
   newB <- shearA %*% b
   
   #put it back together as a linear system, completes foward-elim.
   newA <- matrix(c(newACol1[1,1], newACol1[2,1], newACol2[1,1], newACol2[2,1]), ncol=2)
   
   #back sub.
   u2 = newB[2,1] / newA[2,2]
   u1 = (1/newA[1,1])*(newB[1,1] - u2*newA[1,2])
----

----
Pivoting
 - idea or rearranging the linear solution so a shear may 
   be calc'ed and thereby back sub.
 [0 1    [u1    [1
  1 0] *  u2] =  1]
  - a[1,1] is zero and we can't divide by zero
  - as eq's they would look like
  0*u1 + 1*u2 = 1
  1*u1 + 0*u2 = 1
  - then flip the order of them
  1*u1 + 0*u2 = 1
  0*u1 + 1*u2 = 1
  - and convert back to linear system
 [1 0    [u1    [1
  0 1] *  u2] =  1]
  
 - there is likewise ability to perform 
   column pivoting
 [0 1/2    [u1    [0
  0  0 ] *  u2] =  0]
  -to-
  0*u1 + 1/2*u2 = 0
  0*u1 +  0*u2  = 0
  -flipped as
 1/2*u2 + 0*u1 = 0
  0*u2  + 0*u1 = 0
  -back to linear system
 [1/2 0    [u2    [0
   0  0] *  u1] =  0]
----
   
----
Inverse ops & Maps
 - applying an inverse is a form of undo'ing 
   a linear map
 A*u = b
  -then-
 u = A^-1*b
 
 - given two vectors which are known to map
   to two other vectors, solve for the Matrix
 
 A*v1 = v'1
 A*v2 = v'2
 A*(v1, v2) = v'1,v'2
  -shortened as-
 A*V = V'
 A = V'*V^-1
 - insuch a case the application of matrix
   A is known as "change of basis"
   
 -example
  v1 <- c(1,1)
  v2 <- c(-1,1)
  
  vTick1 <- c(-1,-1)
  vTick2 <- c(1, -1)
  
  V <- matrix(c(v1[1],v1[2],v2[1],v2[2]),ncol=2)
  V2neg1 <- solve(V)
  
  VTick <- matrix(c(vTick1[1],vTick1[2],vTick2[1],vTick2[2]),ncol=2)
  A <- VTick %*% V2neg1
---- 

----
Affine maps 
 - linear map was to map vector to vector
 - affine is to map points to points
  - affine maps are to move and orient
    objects like in visual graphics
  - are composed of components named
   - translation (p): an object is moved
      w/o changing its orientation
   - linear map (A)
  - while linear maps are releated to ratios
    affine maps leave ratios unchanged
   
  x' = p + x1*a1 + x2*a2
  x' = p + A*x
  
  - the x' has the same coordinates 
    in the new system that x did in the 
	former

  - technically the linear map A is 
    applied to the vector x - o
	where o is the (0,0) of the 
	former coordinate system
  x' = p + A*(x - o)
  
 - Translation, as a affine map is written 
   as  x' = p + I*x
----

----
General Affine maps
 - Rotate: have point r and want to rotate
   some pt (x) by (a) degrees
  x' = A*(x - r) + r
  
  -example
  A <- getRotationMatrix(90)
  r <- c(2,1)
  x <- c(3,0)
  
  A %*% (x - r) + r # (3,2)
 
 - Reflect: have line (l) and point (x)
   and want to reflect (x) across line (l)
  - uses the "Foot of a given point" where its
    value (here called (p)) is the mid-pt 
	of the reflection
  p = 1/2*x + 1/2*x'
  x'= 2p - x
----

----
Digression on Statistics 
 - Variance, how wide the obs are spead along an axis
 - Covariance, indicates the positive or negative relationship
    amoung obs 
 - Clustering, divide obs into groups
 - Correlation, measure strength and direction of two
   obs 
 - Probability, P(E) = (Chances for E)/(Total Chances)
   - probability of an Ace in a common deck of cards
     P(Ace) = (4)/(52)
 - Odds, (Chances for E)/(Chances Against E)
   - odds of an Ace is 
     Odds(Ace) = (4) /(52-4)
 - normal distribution, the classic bell-curve
   - also known as Gaussian distribution
 - other distributions 
   - Binomial
   - Inverse Gaussian
   - Log normal
----

----
Simple Linear Regression
 - linear positive slope is also called 
    concordant association
 - linear negative slope is also called
    discordant association
 
 - start with equation y = a * x + b
 - data is a plot so its can be rep'ed as matrix
    .-  -.   .-     -.   
    | y1 |   | x1  1 |   .- -.
    | y2 | = | x2  1 | * | a |
    | .. |   | ..  . |   | b |
    | yn |   | xn  1 |   '- -'
    '-  -'   '-     -'
 - assign each matrix to a symbol
      Y = X * A
 - solve for A
      A = X\Y

 #the first arg is 'formula' - which has it own operators
 #tilde '~' (x7E), right side is the 'response', left side is the 'model'
 LinearModel = lm(SomeData$FieldTwo ~ SomeData$FieldOne, data = SomeData)
 
 #get a summary about the linear regression
 summary(LinearModel)
 
 #get just the coefficents
 LinearModel$coefficients
 
 #get R-squared value
 summary(LinearModel)$r.squared
 
 #plot 5 different esoteric graphs
 plot(LinearModel)
 plot(LinearModel, which=1)

 - to get an idea of the "perfect version" of these esoteric graphs
 #start with a full linear eq
 x<-seq(1,100,0.1)
 
 #f(x) = -2.7x + 6
 y<-2.7*x+6
 
 #add in a little variance
 er<-runif(length(y), min = -1, max = 1)
 y<-y+er
 
 #and plot this hack
 dataxy<-table(rep(x, y))
 LModel<-lm(y~x, data = dataxy)
 plot(LModel)
----
 
----
Multiple Linear Regression
 - start with equation y = b0 + b1*x1 + b2*x2 + ... + bn*xn
 - data is a plot so its can be rep'ed as matrix
    .-  -.   .-                   -.   .-  -.
    | y1 |   | 1 x1,1 x1,2 ... x1,n|   | b0 |
    | y2 | = | 1 x2,1 x2,2 ... x2,n| * | b0 |
    | .. |   | ..                  |   | .. |
    | yn |   | 1 xn,1 xn,2 ... xn,n|   | bn |
    '-  -'   '-                   -'   '-  -'
 - assign each matrix 
      Y = X * B
 - solve for B
    B = (Xᵀ * X)^-1 * Xᵀ * Y
    
  CementData = read.csv("CementData.csv", header = TRUE, sep = ",")
  
  #data as a matrices
  columnOfOnes = rep(1,13)
  x <- as.matrix(columnOfOnes, CementData[,1:4]))
  y <- CementData[,5]
  Beta <- solve(t(x) %*% x) %*% t(x) %*% y
  
  #get the residual of predicted versus observed
  YRegFit <- x %*% Beta[,1]
  Residual= y-YRegFit
  
  #calc the R-squared
  Rsqr = 1 - sum((y - YRegFit)^2)/sum((y - mean(y))^2)
  
  #using builtin R model
  PetroData = read.csv("EscapingHydrocarbons.csv", header = TRUE, sep = ";")
  
  #has fields with long names
  #plus-sign '+' (x2B), is to make the model a multiple linear regression
  MLModel = lm(AmountEscapingHydrocarbons ~ TankTemperature + PetrolTemperature + InitialTankPressure + PetrolPressure, 
               data = PetroData)
----

----
Categorical variables
 - are not from measurements
 - are for classification
 - Nominal Categorical variables: 
  - have two or more categories
  - have no intrinsic order or hierarchy
 - Dichotomous Categorical variables:
  - special case of nominal variables
  - have only two possible values
 - Ordinal Categorical variables,
  - have two or more categories
  - can be ordered and ranked
  
  library(readxl)
  EmployeesSalary <- read_excel("employees.xlsx")
  
  #investigate if a character vector is a category
  unique(EmployeesSalary$LevelOfEmployee)
  
  #character vector reassigned as a factor vector (like an enum)
  EmployeesSalary$LevelOfEmployee <- as.factor(EmployeesSalary$LevelOfEmployee)
  
  #scatter plot by the factor variable
  pch.list <- as.numeric(EmployeesSalary$)
  
  #star '*' (x2A), is to make the model by category
  # formula is now a string literal - text offers no explaination
  LMcat <- lm('Salary~YearsExperience*LevelOfEmployee', data = EmployeesSalary)
----

----
Gradient Descent and Linear Regression
 - pragmatic approach to get the regression 
 - is a supplementary method to the B = (Xᵀ * X)^-1 * Xᵀ * Y
  - the matrix algebra approach will overheat the machine for big data
  - may also be that Xᵀ * X does not exist
 - starts with a cost function that takes a,b and returns an error value
  - error value represents how well the a,b actually fits the data
  
  CostFunction = 1/N * ((SUM(y[i] - (a*x[i] + b))^2)|i=1 to i=n)
   - this is iterated, adjusting a,b each time
   - in 3-D space the CostFunction forms a concave graph
   - since there are two variables, we need a hyperplane along each a,b
    - del is for the partial derivative backward 6 symbol thing
   del/del a = 2/N * (SUM(-x[i] * (y[i] - (a*x[i] + b)))|i=1 to i=n)
   del/del b = 2/N * (SUM(-(y[i] - (a * x[i] + B)))|i=1 to i=n)
   
   - next is the iterative part where
   a[n+1] = a[n] - (eta) * (del/del a[n])
   b[n+1] = b[n] - (eta) * (del/del b[n])
    - eta is the step-size, 
     - too big may over step the min, 
     - too small may take too long
----

----  
Stochastic Gradient Descent
 - like the previous only it works by taking a sample of 
   the data at random
 - likewise imporves performance 
 
  #import stochastic gradient descent lib
  library(sgd, warn.conflicts = FALSE)
  
  #make a data set 
  N <- 10000
  d <- 10
  
  #set the seed for global R random genr'ator
  set.seed(42)
  
  #create a random matrix as explanatory variables
  X <- matrix(rnorm(N*d),ncol=d)
  
  #the estimator
  theta <- rep(5,d+1)
  
  #some random error
  eps <- rnorm(N)
  
  #the response variable
  y <- cbind(1,X) %*% theta + eps
  
  #model with sgd
  dat <- data.frame(y=y, x=X)
  
  #"y ~ ." is the formula where '.' means "and all the rest"
  # 'model="lm"' is the kind of model, also has (glm, cox, gmm, m)
  sgd.theta <- sgd::sgd(y ~ ., data=dat, model="lm")
  
  - Mean Square Error (MSE)
  MSE = 1/n * (SUM((theta_tick - theta)^2)|i=1 to i=n)
  
  myMse <- mean((theta - as.numeric(sgd.theta$coefficients))^2)
  
  #a graph of the the MSE over the iterations
  plot(sgd.theta, theta, type="mse-param")
----
