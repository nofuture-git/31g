Notes Natural Language Processing
----
Python Natural Language Processing
By: Jalaj Thanaki
Publisher: Packt Publishing
Pub. Date: July 31, 2017
Web ISBN-13: 978-1-78728-552-1
Print ISBN-13: 978-1-78712-142-3
Pages in Print Edition: 486
----

----
https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html
https://www.cs.bgu.ac.il/~michaluz/seminar/CKY1.pdf
----

----
Basics
 - is a branch of AI
 - handles specific problems related to
  - speech recognition
  - Q/A 
  - language translation
  - text summarization
  - sentiment analysis
  - chatbots
  - text classification
  - topic segmentation
  
  - Natural Language Understanding (NLU)
   - considered first component of NLP
   - is the process of turning input into 
     its computation representation
   - requires analysis of
    - morphological
    - lexical
    - syntatic
    - semantic
    - handle ambiguity
    - discourse integration
    - pragmatic
  
  - Natural Language Generation (NLG)
   - considered second component of NLP
   - the creation of natual language from the machine
    - in machina exspriavit
    
  - Corpus, collection of written or spoken natural language material
   - plural is Corpora
   - available corpus
    - Google Books Ngram corpus
    - Brown corpus
    - American National corpus
  
  - other data sources
  https://github.com/caesar0301/awesome-public-datasets.
  https://www.kaggle.com/datasets.
  https://www.reddit.com/r/datasets/.
  
  - pre-packed web-scrappers with all major langs
  
  -install the package nltk
  python -m pip install --proxy WINDOWSDOMAIN\user.name:somePassword@corpproxy.domain.com:9090 nltk
  
  - download the corpora directly from http://www.nltk.org/nltk_data/
  - place corpus zip files into 
  C:\nltk_data\corpora
  - place "punkt" as unzipped content into
  C:\nltk_data\tokenizers
----

----
Data Attributes
 - Categorical (Qualitative)
  - Ordinal:
     - like adverbs, best, good, bad, worst
     - express a quality or circumstance     
  - Nominal:
     - categorical name or grouping
 - Numeric (Quantitative)
  - Continuous, 
  - Discrete
     - numerical values taken from a limited
       set - like enums
 - NLP deals mostly with Categorical
----

----
Preprocessing
 - get the data in the preferred format
 - clean the data of missing values, encoding errors, etc
 - sample the data to get a sense of its content
 - transform the text data to numeric data
----

----
Context-Free Grammer
 - has four components
  - a set of non-terminal symbols (N)
  - a set of terminal symbols (T)
  - a start symbol (S) which is a part of (N)
  - a set of production rules (P) for sentence creation
  
 - is an attempt to generalize a sentence outside of any
   language - an abstraction
 - sounds like the kind of thing L.W. advised you can't really do
----
  
----
Morphological Analysis
 - determine the kind of affixes which can be applied
   to a stem and\or root
 - stemming and  lemmatization is to reduce the inflectional 
   and derivational forms of words to a common base
 - Morphemes, an atomic unit of linguistics
  - Free
  - Bound
   - Derivational, infixes to a root which change its semantic meaning
       happy (adjective) -> happiness (noun)
   - Inflectional, in the form of suffix, prefix and infix on nouns, pronouns,
      adjectives and articles
     - do not change the semantic meaning but express tense, mood, aspect, etc.
 - Word, smallest meaningful unit of a sentence
 - Stem, the part of a word which can be operated on with some kind of afix
   - is a root plus derivational morphemes
 - Lemma, the inflected form of the a word
 - Root, a morpheme which can be a word (i.e. stand on its own)
   - cannot be further divided
  "unexpected"
  - stem: expect
  - afixes: 
   - prefix: un-
   - suffix: -ed
  - morphemes: (3 total) un-, expect, -ed
  - root: expect (because it can stand on its own)
  
  "laudare" (Latin)
   - lemma: laudare, laudo (I praise)
   - stem(s): lauda (present), laudav (perfect), laudat (supine)
   - root: laud
  
  - in Semitic langs (Hebrew) the root is the word without and
     diacritics (vowels)
----  

----
Lexical Analysis
 - a word level analysis
 - also-known-as tokenization, or finding the word-boundaries
 - Token, the meaningful elements created by lexical analysis
 - part-of-speech tag, are labels for the kinds of words, 
    nouns, verbs, adjectives, etc.
 - lemmatization, identify the correct POS and meaning of words
    in a sentence
    - is contextually
----

----
Syntactic Analysis
 - concerned with the structure of words into sentences and phrases
  - to convey logical order
  - is defined by rules of grammar
----

----
Semantic Analysis
 - concerned with meaning between words and to what they refer
 - can be applied at higher groupings like sentences, paragraphs,
    chapters and sections or even whole documents
 - hypernym and hyponyms
  - is similar to the divide of common nouns into 
     collective and abstract
  - "color" is to hypernym what "red", "green", "blue" is to hyponym
 - homonym, words that sound alike are are spelled alike but have 
    totally different meanings
   - "bear" the animal and "bear" the verb
 - polyseme, one word with different but related meaning
   - is a vague concept
   - example is "to get" 
     - as in "I'll get drinks" (procure), 
     - "she gets scared" (become),
     - "I get it" (understand)
 - metonym, a figure of speech in which a thing or concept is referred to by 
    the name of something closely associated with that thing or concept
 - anaphora, a word whose semantic meaning is dependent on a word
   which appeared before it
 - cataphora, same as anaphora only its after it, not before it
 - endophora,
  - more general form where meaning is derived from 
    somewhere else in the linguistic context
  - anaphora and cataphora are specific cases of this
 - exophora, when the meaning cannot be understood from the linguistic context 
 - deictic, based on a distinction between semantic meaning and denotational meaning
    where denotational meaning is dependent on time-space.
----

----
Ambiguity
 - meaning is not clear
 - can happen at any level
  - lexical ambiguity
   "Look at the stars" (verb) and "She gave him a warm look" (noun)
  - syntactic ambiguity
   "The man saw the girl with the telescope" who has the telescope?
    - also known as prepositional phrase ambiguity
    - has a known formula for solving
     F(v, n, p) = log((p*(p/v) / (p*(p/n))))
      - p is preposition, v is verb, n is noun
      - p*(p/v) probability of seeing the preposition after the verb
      - p*(p/n) probability of seeing the preposition after the noun
  - semantic ambiguity
   "Pope's baby steps on homosexuality"
    - 'baby-steps' would have made this more clear
  - pragmatic ambiguity
   "Stand over there" where 'there' may only have sense from a previous
     sentence or by someone actually pointing    
----

----
Sentence Tokenization
 - break a document into sentences
 
 #get dependencies
 import nltk
 from nltk.corpus import gutenberg as cg
 
 #read a text from the corpus
 busterBrown = cg.raw("burgess-busterbrown.txt")
 
 #get a sample of it
 rawContent = busterBrown[0:1000]
 
 #pass it to the sentence tokenizer
 toSentences = nltk.tokenize.sent_tokenize(rawContent)
 
 #deals with the line breaks and what not - real easy
 print(toSentences[1])
  
 #challenging sentences
 challengeExample00 = nltk.tokenize.sent_tokenize(
 """He has completed his Ph.D. degree.  
 He should be happier than this.""")
 
 #tokenizer figures out sentence ends at 'degree.' and not after D in 'Ph.D.'
 challengeExample00[0]
 
 #word tokenizer
 from nltk.tokenize import word_tokenize
 words2tokens = word_tokenize("""Word tokenization is defined 
 as the process of chopping a stream of text up into words, 
 phrases, and meaningful strings.""" )
 [print(s) for s in words2tokens]
----

----
Stemming & Lemmatization raw text
 - Stemming: 
  - does NOT consider the part-of-speech tag
  - tends to be a cruder process of just chops off endings
  - best algo, empirically, is Porter's algorithm
 - Lemmatization:
  - the proper way of taking context into account
  - directly targets and removes inflectional endings
  - gets the base, dictionary form of a word
  - requires a lot more resources being full vocab and
    morphological analysis
 
 from nltk.stem import PorterStemmer
 port = PorterStemmer()
 #prints 'love'
 print(port.stem("loving"))
 
 from nltk.stem import WordNetLemmatizer
 lemmatizer = WordNetLemmatizer();
 #prints 'funny'
 lemmatizer.lemmatize("funnier", pos='a')
 #prints 'good'
 lemmatizer.lemmatize("better", pos='a')
----

----
Stopwords
 - common occuring words
 
 from nltk.corpus import stopwords
 stopwordList = stopwords.words('english')
 [print(s) for s in stopwordList]
----

----
NLP Parsers
 - two major approaches
  - top-down
   - is the more familar where the parser is reading
     a stream from left to right
   - has backtracking where it goes foward so far, finds
     its on the wrong rule, goes back to and starts foward again
  - bottom-up
   - takes each word as a starting point, finds a rule
     to match each on, the starts combining them upwards

 - Proability Context-Free Grammar
  - like a static grammar 
  - differs regarding the choice pattern where
    each possible choice is given a percent 
    in which all the percents add up to 1 
   - example
   nounPhrase : nounPhrase nounPhrase // 0.1
              | nounPhrase prepositionalPhrase //0.2
              | NOUN //0.7
              ;
  - each possible combination will have a calc'able 
    tree-probability
  
 - the Cocke-Kasami-Younger algorithm
  - a bottom up algo
  - gives result in cubic time
  
 #the test word
 word = "aaabb"
 
 #the grammar
 mystring = "S -> e | AB | XB, T -> AB | XB, X -> AT, A -> a, B -> b"
 
 #grammar parsed to dict
 grammar = dict([(l[0].strip(), [ll.strip() for ll in l[1].split("|")]) 
 for l in [r.split("->") for r in mystring.split(",")]])
 
 #the cky cube (where each "cell" at [i,j] is itself a list)
 myCube = []
 for i in range(len(word)): myCube.append([[] for j in range(len(word))])
 
 #append the diagonals of the cube
 chars = list(word)
 for i in range(len(word)):
   c = chars[i]
   for key in grammar:
     key2Val = grammar[key]
     if len(key2Val) == 1 and key2Val[0] == c:
       myCube[i][i].append(key)
 
 #the Cocke-Kasami-Younger algorithm
 for j in range(2, len(word)+1):
   for i in range(j-2, -1, -1):
     for k in range(i+1, j):
       toTheLeft = myCube[i][k-1]
       directBelow = myCube[k][j-1]
       for left in toTheLeft:
         for below in directBelow:
           if left == below:
             continue
           toMatch = "{0}{1}".format(left, below)
           for key in grammar:
             key2Val = grammar.get(key)
             if toMatch in key2Val:
               myCube[i][j-1].append(key)
               
   |  1  |  2  |  3  |  4  |  5  |
   |  a  |  a  |  a  |  b  |  b  |
   +-----+-----+-----+-----+-----+
 0 |  A  |     |     |     |   X |
   +-----+-----+-----+-----+-----+
 1 |     |  A  |     |   X | S,T |
   +-----+-----+-----+-----+-----+
 2 |     |     |  A  | S,T |     |
   +-----+-----+-----+-----+-----+
 3 |     |     |     |  B  |     |
   +-----+-----+-----+-----+-----+
 4 |     |     |     |     |  B  |
   +-----+-----+-----+-----+-----+
 
 - the Stanford NLP Parser
  - download the Java jars from https://stanfordnlp.github.io/CoreNLP/
  - this is half-a-gig 
  - the CoreNLP modules need to be in box's ClassPath
  - also need a python package named pycorenlp
  - contains pretty much everything in its output to turn
    unstruct'ed data in something more computational
  - allows for parser re-training using some domain-specific tagged corpus
----
  
 
  
  
  