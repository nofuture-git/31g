Notes Natural Language Processing
----
Python Natural Language Processing
By: Jalaj Thanaki
Publisher: Packt Publishing
Pub. Date: July 31, 2017
Web ISBN-13: 978-1-78728-552-1
Print ISBN-13: 978-1-78712-142-3
Pages in Print Edition: 486
----

----
https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html
https://www.cs.bgu.ac.il/~michaluz/seminar/CKY1.pdf
https://arxiv.org/pdf/1411.2738.pdf
https://ronxin.github.io/wevi/
----

----
Basics
 - is a branch of AI
 - handles specific problems related to
  - speech recognition
  - Q/A 
  - language translation
  - text summarization
  - sentiment analysis
  - chatbots
  - text classification
  - topic segmentation
  
  - Natural Language Understanding (NLU)
   - considered first component of NLP
   - is the process of turning input into 
     its computation representation
   - requires analysis of
    - morphological
    - lexical
    - syntactic
    - semantic
    - handle ambiguity
    - discourse integration
    - pragmatic
  
  - Natural Language Generation (NLG)
   - considered second component of NLP
   - the creation of natural language from the machine
    - in machina exspriavit (ghost in the machine)
    
  - Corpus, collection of written or spoken natural language material
   - plural is Corpora
   - available corpus
    - Google Books Ngram corpus
    - Brown corpus
    - American National corpus
  
  - other data sources
  https://github.com/caesar0301/awesome-public-datasets.
  https://www.kaggle.com/datasets.
  https://www.reddit.com/r/datasets/.
  
  - pre-packed web-scrappers with all major langs
  
  -install the package nltk
  python -m pip install --proxy WINDOWSDOMAIN\user.name:somePassword@corpproxy.domain.com:9090 nltk
  
  - download the corpora directly from http://www.nltk.org/nltk_data/
   - https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip
   - https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip
  - place corpus zip files into 
  C:\nltk_data\corpora
  - place "punkt" as unzipped content into
  C:\nltk_data\tokenizers
----

----
Data Attributes
 - Categorical (Qualitative)
  - Ordinal:
     - like adverbs, best, good, bad, worst
     - express a quality or circumstance     
  - Nominal:
     - categorical name or grouping
 - Numeric (Quantitative)
  - Continuous, 
  - Discrete
     - numerical values taken from a limited
       set - like enums
 - NLP deals mostly with Categorical
----

----
Preprocessing
 - get the data in the preferred format
 - clean the data of missing values, encoding errors, etc
 - sample the data to get a sense of its content
 - transform the text data to numeric data
----

----
Context-Free Grammar
 - has four components
  - a set of non-terminal symbols (N)
  - a set of terminal symbols (T)
  - a start symbol (S) which is a part of (N)
  - a set of production rules (P) for sentence creation
  
 - is an attempt to generalize a sentence outside of any
   language - an abstraction
 - sounds like the kind of thing L.W. advised you can't really do
----
  
----
Morphological Analysis
 - Morphemes, an atomic unit of linguistics
  - Free
  - Bound
   - Derivational, infixes to a root which change its semantic meaning
       happy (adjective) -> happiness (noun)
   - Inflectional, in the form of suffix, prefix and infix on nouns, pronouns,
      adjectives and articles
     - do not change the semantic meaning but express tense, mood, aspect, etc.
     - Conjugation: inflection of verbs
     - Declension: inflection of everything else
     - some kinds of inflection 
      - Tense: as in the location in time (past, present, future)
      - Aspect: fabric of time as in single block, continuous flow or repetitive
      - Mood: for the speaker to express their attitude toward what they are saying
         expressing things like necessity, ability, wishing
      - Person: 1st, 2nd, 3rd
      - Voice: relation between an action and its participants
         being like "this did that" (active) or "that happened to this" (passive)
      - Case: for indicating a grammatical function (in english I, me, my)
         these may intermingle with Number like in Latin (1DSN, 3DPD, etc.)
      - Gender: feminine, masculine
      - Number: singular, dual, plural
 - Word, smallest meaningful unit of a sentence
 - Stem, the part of a word which can be operated on with some kind of afix
   - is a root plus derivational morphemes
 - Lemma, the inflected form of the a word
 - Root, a morpheme which can be a word (i.e. stand on its own)
   - cannot be further divided
   - Radical, each letter of the root
  "unexpected"
  - stem: expect
  - afixes: 
   - prefix: un-
   - suffix: -ed
  - morphemes: (3 total) un-, expect, -ed
  - root: expect (because it can stand on its own)
  
  "laudare" (Latin)
   - lemma: laudare, laudo (I praise)
   - stem(s): lauda (present), laudav (perfect), laudat (supine)
   - root: laud
  
 - in Semitic langs (Hebrew, Arabic, Syraic) the root is typically
   three constants and then its applied to known patterns 
   (aka transfix, nonconcatenative, root-and-pattern)
  - example: the pattern is _a_a_  so the root of k.t.b applied
    would be katab
   - certain patterns may have specific meanings like ma__a_ as the
     pattern for "geographic location"
 
----  

----
Lexical Analysis
 - a word level analysis
 - also-known-as tokenization, or finding the word-boundaries
 - Token, the meaningful elements created by lexical analysis
 - part-of-speech tag, are labels for the kinds of words, 
    nouns, verbs, adjectives, etc.
 - lemmatization, identify the correct POS and meaning of words
    in a sentence
    - is contextually
----

----
Syntactic Analysis
 - concerned with the structure of words into sentences and phrases
  - to convey logical order
  - is defined by rules of grammar
 - Constituency Grammar: classic form of grammar analysis based on 
    subject-predicate division
 - Dependency Grammar: newer idea, 
  - deny subject-predicate division
  - root the clause at the verb
  - forms a surjective directed tree graph
   - a graph because its made up of nodes and edges
   - a tree because it has one root node
   - directed because the edges are pointing in one direction
   - surjective because the root node has an outdegree above 1
    - outdegree is the number of edges originating out of a node
      (think color-book sun with rays)
  - Universal Dependencies: recent spec between Google and Stanford
    to have a finite set of node-types used in all possible langs
    (e.g. nsubj is the label for nominal subject)
    - any given sentence of any given lang can be parsed to a
      dependency graph where each morph (word) is one of these
      finite labels
----

----
Semantic Analysis
 - concerned with meaning between words and to what they refer
 - can be applied at higher groupings like sentences, paragraphs,
    chapters and sections or even whole documents
 - hypernym and hyponyms
  - is similar to the divide of common nouns into 
     collective and abstract
  - "color" is to hypernym what "red", "green", "blue" is to hyponym
 - homonym, words that sound alike are are spelled alike but have 
    totally different meanings
   - "bear" the animal and "bear" the verb
 - polyseme, one word with different but related meaning
   - is a vague concept
   - example is "to get" 
     - as in "I'll get drinks" (procure), 
     - "she gets scared" (become),
     - "I get it" (understand)
 - metonym, a figure of speech in which a thing or concept is referred to by 
    the name of something closely associated with that thing or concept
 - meronym, a word which is a part of something but is used to refer to the whole 
  - e.g. "see many familiar faces" where "faces" is meronym for "people"
 - anaphora, a word whose semantic meaning is dependent on a word
   which appeared before it
 - cataphora, same as anaphora only its after it, not before it
 - endophora,
  - more general form where meaning is derived from 
    somewhere else in the linguistic context
  - anaphora and cataphora are specific cases of this
 - exophora, when the meaning cannot be understood from the linguistic context 
 - deictic, based on a distinction between semantic meaning and denotational meaning
    where denotational meaning is dependent on time-space.
 - orthography, set of conventions for writing a language including spelling, 
   punctuation, etc.
 - diglossia, when a community has two languages for different conditions
    biggest example is Arabic which is written in Modern Standard Arabic (MSA)
    but not even one community actually speaks in it.
 - clitic, a morpheme which depends phonologically on another word 
 - gemination, a spoken consonant for an audibly longer period of 
   time than that of a short consonant
 - homograph, two or more words spelled the same but not necessarily pronounced the same
   having different meanings and origins
----

----
Ambiguity
 - meaning is not clear
 - can happen at any level
  - lexical ambiguity
   "Look at the stars" (verb) and "She gave him a warm look" (noun)
  - syntactic ambiguity
   "The man saw the girl with the telescope" who has the telescope?
    - also known as prepositional phrase ambiguity
    - has a known formula for solving
     F(v, n, p) = log((p*(p/v) / (p*(p/n))))
      - p is preposition, v is verb, n is noun
      - p*(p/v) probability of seeing the preposition after the verb
      - p*(p/n) probability of seeing the preposition after the noun
  - semantic ambiguity
   "Pope's baby steps on homosexuality"
    - 'baby-steps' would have made this more clear
  - pragmatic ambiguity
   "Stand over there" where 'there' may only have sense from a previous
     sentence or by someone actually pointing    
----

----
Sentence Tokenization
 - break a document into sentences
 
 #get dependencies
 import nltk
 from nltk.corpus import gutenberg as cg
 
 #read a text from the corpus
 busterBrown = cg.raw("burgess-busterbrown.txt")
 
 #get a sample of it
 rawContent = busterBrown[0:1000]
 
 #pass it to the sentence tokenizer
 toSentences = nltk.tokenize.sent_tokenize(rawContent)
 
 #deals with the line breaks and what not - real easy
 print(toSentences[1])
  
 #challenging sentences
 challengeExample00 = nltk.tokenize.sent_tokenize(
 """He has completed his Ph.D. degree.  
 He should be happier than this.""")
 
 #tokenizer figures out sentence ends at 'degree.' and not after D in 'Ph.D.'
 challengeExample00[0]
 
 #word tokenizer
 from nltk.tokenize import word_tokenize
 words2tokens = word_tokenize("""Word tokenization is defined 
 as the process of chopping a stream of text up into words, 
 phrases, and meaningful strings.""" )
 [print(s) for s in words2tokens]
----

----
Stemming & Lemmatization raw text
 - Stemming: 
  - does NOT consider the part-of-speech tag
  - tends to be a cruder process of just chops off endings
  - best algo, empirically, is Porter's algorithm
 - Lemmatization:
  - the proper way of taking context into account
  - directly targets and removes inflectional endings
  - gets the base, dictionary form of a word
  - requires a lot more resources being full vocab and
    morphological analysis
 
 from nltk.stem import PorterStemmer
 port = PorterStemmer()
 #prints 'love'
 print(port.stem("loving"))
 
 from nltk.stem import WordNetLemmatizer
 lemmatizer = WordNetLemmatizer();
 #prints 'funny'
 lemmatizer.lemmatize("funnier", pos='a')
 #prints 'good'
 lemmatizer.lemmatize("better", pos='a')
----

----
Stopwords
 - common occuring words
 
 from nltk.corpus import stopwords
 stopwordList = stopwords.words('english')
 [print(s) for s in stopwordList]
 expectedWords = [
    'i', 'me', 'my', 'myself', 'we', 'our', 
    'ours', 'ourselves', 'you', "you're", 
    "you've", "you'll", "you'd", 'your', 
    'yours', 'yourself', 'yourselves', 'he', 
    'him', 'his', 'himself', 'she', "she's", 
    'her', 'hers', 'herself', 'it', "it's", 
    'its', 'itself', 'they', 'them', 'their', 
    'theirs', 'themselves', 'what', 'which', 
    'who', 'whom', 'this', 'that', "that'll", 
    'these', 'those', 'am', 'is', 'are', 'was', 
    'were', 'be', 'been', 'being', 'have', 
    'has', 'had', 'having', 'do', 'does', 
    'did', 'doing', 'a', 'an', 'the', 'and', 
    'but', 'if', 'or', 'because', 'as', 
    'until', 'while', 'of', 'at', 'by', 'for', 
    'with', 'about', 'against', 'between', 
    'into', 'through', 'during', 'before', 
    'after', 'above', 'below', 'to', 'from', 
    'up', 'down', 'in', 'out', 'on', 'off', 
    'over', 'under', 'again', 'further', 
    'then', 'once', 'here', 'there', 'when', 
    'where', 'why', 'how', 'all', 'any', 
    'both', 'each', 'few', 'more', 'most', 
    'other', 'some', 'such', 'no', 'nor', 'not', 
    'only', 'own', 'same', 'so', 'than', 'too', 
    'very', 's', 't', 'can', 'will', 'just', 
    'don', "don't", 'should', "should've", 
    'now', 'd', 'll', 'm', 'o', 're', 've', 
    'y', 'ain', 'aren', "aren't", 'couldn', 
    "couldn't", 'didn', "didn't", 'doesn', 
    "doesn't", 'hadn', "hadn't", 'hasn', 
    "hasn't", 'haven', "haven't", 'isn', "isn't", 
    'ma', 'mightn', "mightn't", 'mustn', "mustn't", 
    'needn', "needn't", 'shan', "shan't", 
    'shouldn', "shouldn't", 'wasn', "wasn't", 
    'weren', "weren't", 'won', "won't", 'wouldn', 
    "wouldn't"]
 
----

----
NLP Parsers
 - two major approaches
  - top-down
   - is the more familiar where the parser is reading
     a stream from left to right
   - has backtracking where it goes forward so far, finds
     its on the wrong rule, goes back to and starts forward again
  - bottom-up
   - takes each word as a starting point, finds a rule
     to match each on, the starts combining them upwards

 - Probability Context-Free Grammar
  - like a static grammar 
  - differs regarding the choice pattern where
    each possible choice is given a percent 
    in which all the percents add up to 1 
   - example
   nounPhrase : nounPhrase nounPhrase // 0.1
              | nounPhrase prepositionalPhrase //0.2
              | NOUN //0.7
              ;
  - each possible combination will have a calc'able 
    tree-probability
  
 - the Cocke-Kasami-Younger algorithm
  - a bottom up algo
  - gives result in cubic time
  
 #the test word
 word = "aaabb"
 
 #the grammar
 mystring = "S -> e | AB | XB, T -> AB | XB, X -> AT, A -> a, B -> b"
 
 #grammar parsed to dict
 grammar = dict([(l[0].strip(), [ll.strip() for ll in l[1].split("|")]) 
 for l in [r.split("->") for r in mystring.split(",")]])
 
 #the cky cube (where each "cell" at [i,j] is itself a list)
 myCube = []
 for i in range(len(word)): myCube.append([[] for j in range(len(word))])
 
 #append the diagonals of the cube
 chars = list(word)
 for i in range(len(word)):
   c = chars[i]
   for key in grammar:
     key2Val = grammar[key]
     if len(key2Val) == 1 and key2Val[0] == c:
       myCube[i][i].append(key)
 
 #the Cocke-Kasami-Younger algorithm
 for j in range(2, len(word)+1):
   for i in range(j-2, -1, -1):
     for k in range(i+1, j):
       toTheLeft = myCube[i][k-1]
       directBelow = myCube[k][j-1]
       for left in toTheLeft:
         for below in directBelow:
           if left == below:
             continue
           toMatch = "{0}{1}".format(left, below)
           for key in grammar:
             key2Val = grammar.get(key)
             if toMatch in key2Val:
               myCube[i][j-1].append(key)
               
   |  1  |  2  |  3  |  4  |  5  |
   |  a  |  a  |  a  |  b  |  b  |
   +-----+-----+-----+-----+-----+
 0 |  A  |     |     |     |   X |
   +-----+-----+-----+-----+-----+
 1 |     |  A  |     |   X | S,T |
   +-----+-----+-----+-----+-----+
 2 |     |     |  A  | S,T |     |
   +-----+-----+-----+-----+-----+
 3 |     |     |     |  B  |     |
   +-----+-----+-----+-----+-----+
 4 |     |     |     |     |  B  |
   +-----+-----+-----+-----+-----+
 
 - the Stanford NLP Parser
  - download the Java jars from https://stanfordnlp.github.io/CoreNLP/
  - this is half-a-gig 
  - the CoreNLP modules need to be in box's ClassPath
  - also need a python package named pycorenlp
  - contains pretty much everything in its output to turn
    unstruct'ed data in something more computational
  - allows for parser re-training using some domain-specific tagged corpus
----
  
----
Tagging and POS Taggers
 - words can change from nouns to verbs, etc.
 - what part-of-speech tag they take depends on both the context and def.
 - could be applied at a phrase or clause level
 - parts of speech may be a mixture where what's tagged always per-word
  - multiword expression is like a chicle or words which when joined by 
    a hyphen would help clarify
  - name entity recognition, where the tagger should recognize a group 
    of words as a kind of proper noun (e.g. North Dakota)
   - name entity recog. may classify names into abstract groups
    - e.g. Location, Person, Org
----

----
n-gram 
 - is a continuous sequence of n items from the given sequence of text or speech data
 - the items could be phonemes, syllables, letters, words, etc.
  
 - unigram (n=1)
  "this is a pen" : this, is, a, pen
 - bigram (n=2)
  "this is a pen" : 'this is', 'is a', 'a pen'
 - trigram (n=3)
  "this is a pen" : 'this is a', 'is a pen'
  
 from nltk import ngrams
 n = 4
 rslts = ngrams('this is a pen', n)
----

----
Bag-of-words
 - basically all the unique words present in the corpus
 - can be used to get frequency counts
 - the word order is not important
 - would technically be unigram when items are words
----

----
Term Frequency - Inverse Document Frequency (tf-idf)
 - uses linear algebra cosine similarity and Euclidean distance to 
   find words of similar semantic meaning.
 TF(t) = (Number of times 't' appears in a document)/(Total number of terms in a document)
 IDF(t)= log10(Total number of documents / Number of document with term 't' in it) 
 
 TF(t)*IDF(t) is the final formula
  
 #example of basics
 def wordCound(sentence, word):
     words = sentence.split()
     return len([x for x in words if x == word])
      
 def docCount(sentences, word):
     return sum(1 for sentence in sentences if wordCound(sentence, word) > 0)
      
 def tf(sentence, word):
     den = len(sentence.split())
     num = wordCound(sentence, word)
     return num/den
     
 def idf(sentences, word):
     x = docCount(sentences, word)
     return math.log10(len(sentences) / (x if x else 1))

----

----
Vectorization
 - map every possible word to a specific integer.
 - idea is the machines can't "understand" text
 - words are turned into vectors, sentences to matrices
  - can represent a word as a sorta bitwise OR where the total
    length is for every possible word and the given word is an
    equal length array with a one in the matching position.
  - one-hot vector, a vector in which all positions are zero except 
    for one of them
  mySentence = 'here is my sentence'
  myWords = mySentence.split()
  myWordIndices  = [x + 1 for x in range(len(myWords))]
  #[1, 2, 3, 4]
  myVector4Word = [1,0,0,0] #the word 'here'
----

----
Language Model
 - to calc the probability of the next word by 
   observing the previous word sequence
 - Markov Assumption
  - idea that only the last couple of words are needed
    to predict the next word
 - basic unigram calc
  - given last known word w[i-1] and guess for next word 
    as w[i]
  - get a count of how many times w[i-1] and w[i] appear 
    together
  - then get a count of how many time w[i-1] occurs in total
  - divide the former by the later
  
  P(w[i] | w[i-1]) = count(w[i-1],w[i]) / count(w[i-1])
----

----
Word2Vec
 - deals with distributional semantics 
  - other semantics are lexical, formal, compositional
 - Distributional Hypothesis, the idea of a correlation between 
    distributional and semantic similarity.  Words can be understood
    by what other words often appear near them.
 - its overall output is assigning each word some vector in 
   a vector space
   - length of a vector is called magnitude
   - since a vector's components are the rise & run
     the vector's length may be assigned as the 
     hypotenuse of a right triangle
 - aim to convert every word into a vector format, such
   that they are good a predicting the words that appear
   in their context

 - arch. diagram
 
  Corpus  ----------------> Vocabulary (algo Lossy Counting)
  (input)                    Builder
    |                          |
    |                       Vocabulary (output)
    |                          |
    |  Sentence Windows        |
    |     (input)              V
    +---------------------> Context (algo Dynamic Context Window,
                            Builder       Subsampling, Pruning) 
                               |
        (algo CBOW)            |       (algo Skip-gram)
       +-----------------------+-----------------------+
       |                                               |
       |               [2-layer Neural Net.]           |
       |       Input         Hidden        Output      |
       |       Layer          Layer        Layer       |
       |           +--------+                          |
       V        x1 | \   /> |   h1   \   />  y1        V
     Input----> x2 |  \ /   |   h2    \ /    y2 <----Output
     Words      x3 |   x    |   ..     x     y3      Words
                .. |  / \   |   hj    / \    .. 
                xg | /   \> |   ..   /   \>  yi 
                .. +--+-----+   hN       |   .. 
                xV ^  |                  |   yV 
                   |  |                  |
                   |  +------------------|---------> Vectors
                   |                     |           (output)
                   +---------Parameter---+
                              Learner  (algo Backpropagation, 
                                             Hierarchical Softmax,
                                             Negative Sampling)
 - Vocabulary Builder, gets a list of distinct words 
    present in the corpus
    - mapped to a unique id index
    - and a count thereof 
    - Lossy Counting is where a min threshold must be 
      met to be part of the output
    
 - Context Builder, sliding window of for words input
   - given a context word and context window of 5 
     would get 5 words to the left of context word and
     5 to the right of the context word
    - this is the context window
   - output is word tuples of words
    - iterate each word (the context word)
    - gathering the words to the left and right (window size)
    - tuple them up
    - add the tuple to an output list
    - go to next word, etc.
   - Dynamic Context Window, weigh words by distance from center
     where 5/5 is center word, 4/5 is direct left\right and so on
   - Subsampling, arbitrary frequency threshold with probability
     p = 1 - sqrt(t/tf) where t is arbitrary between 5 and 10
   - Pruning, both removes less-frequent words and truncates the
     total size.   
  
 - 2-layer Neural Net.
  - basically just matrix algebra
  - there are only two exogenous inputs
   - Skip-Gram 
    - input vector [0,1,0,0,0,0,0] (X)
    - output vector [1,0,0,1,0,0,0] (Y_actual)
    - input is one word, output is two context words
   - CBOW
    - input vector [0.5,0,0,0.5,0,0,0] (X)
    - output vector [0,1,0,0,0,0,0] (Y_actual)
    - just like Skip-Gram except input\output
      gets swapped and input gets reduced to 
      average so SUM is '1'
  - there are two endogenous inputs
   - WI is a V x N matrix which starts 
     with all random values which are less than
     0.1
   - WO is the same expect its N x V
  - has two general steps per word (forward & backpropagate)
   - forward, this is the estimate
    Y_estimate = softmax(X * WI * WO)
   - backpropagate 
    - t(blah) means Transpose
    - eta is learning rate
    E = Y_actual - Y_estimate
    H = X * WI
    NG = WO * t(E)
    OG = t(E) * H
    IG = NG * X
    WO = WO - t(OG * eta)
    WI = WI - t(IG * eta)
----

----
Basics of Neuron
 - in practice, takes three inputs total 
 - first two inputs are of equal len,
  - both are ordered list
  - one is values, the other is weights
 - let u = sum(x[i] * w[i]) | i=0 to K
 - third input is "activation function"
  - activation function takes 'u' as its input
  - typical activation functions are 
   - Step Function
    f(u) = u > 0 ? 1 : 0 
   - Sigmoid Function
    f(u) = 1 / (1 + e^(u*-1))
    
 - becomes multilayer when f(u) is 
   passed to another neuron as, I assume,
   scalar multiplication of this other 
   neuron's weights 
  - this is likewise summed
  - and the sum is passed to this 
    latter neuron's activation function
----

----
Error Function
 - also known as "Loss Function"
 - are typically of two mathematical forms
  L1 Least Absolute Deviation
  L2 Least Square Error
 - L2 is used here
  E = 1/2*(t - y)^2
   t: target vector
   y: estimated vector
  
 - Gradient Descent
  - taking the partial derivative of E with respect to y
  dev(E)/dev(y) = y - t
  - y is dependent on f(u) and f(u) is dependent on WI (weights)
  - apply "partial derivative chain rule"
  dev(E)/dev(u) = dev(E)/dev(y) * dev(y)/dev(u)
                = (y - t) * y(1 - y)
                
  dev(E)/dev(WI) = dev(E)/dev(y) * dev(y)/dev(u) * dev(u)/dev(WI)
                 = (y - t) * y(1 - y) * x[i]
---- 