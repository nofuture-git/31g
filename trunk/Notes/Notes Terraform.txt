Notes Terraform
----

----
Terraform: Up & Running, 2nd Edition
by Yevgeniy Brikman
Publisher: O'Reilly Media, Inc.
Release Date: September 2019
ISBN: 9781492046905

code examples: https://github.com/brikis98/terraform-up-and-running-code
----

----
Terraform basics
 - is written in Go
 - performs API calls to a known cloud provider (AWS, Azure, 
   Google Cloud, DigitalOcean, OpenStack and more)
 - is a single .exe in Windows without any installer 
  - needs to be added to the PATH variable
 - uses environment variables for AWS IAM users
 [System.Environment]::SetEnvironmentVariable("AWS_ACCESS_KEY_ID", `
                                              "ABCDEFGHIJKLMNOPQRS", `
                                              [System.EnvironmentVariableTarget]::Process)
                                              
 [System.Environment]::SetEnvironmentVariable("AWS_SECRET_ACCESS_KEY", `
                                              "c3VwZXJfL1NlY3JldF9uZWVkc1RvQmUxMTExMTExMTE=", `
                                              [System.EnvironmentVariableTarget]::Process)
                                              
 - Terraform code is a syntax named HashiCorp Configuration Language (HCL)
 - is expected file extension of .tf
 - documentation at https://registry.terraform.io/providers/hashicorp/aws/latest/docs
 - terraform.exe will look for a file named 'main.tf" in the working directory
 - typical ignore files of Terraform are: .terraform *.tfstate *.tfstate.backup
----

----
Infrastructure as Code (IAC)
 - idea of how code is hosted, managed, deployed and monitored 
   is all considered a software problem is is handled as code
   itself
 - benefits include:
  - self-service: by the developers without assistance from a sysadmin
  - speed: steps as code will run faster than any human user
  - safety: steps as code are not prone to typos and manual errors
  - documentation: the infrastructure is clearly defined in as code
    and not hindered by tribal knowledge
  - version control: infrastructure changes are under source control
    with logs, commit comments, user names, etc.
  - validation: IAC can be code reviewed
----
 
----
Ad hoc scripts
 - IAC as bash, python, powershell scripts 
  - tends to lack a standard API for standard IAC tasks
  - quickly becomes overwhelming 
----

----
Configuration Management Tools 
 - IAC as applications to install and manage software on existing servers
 - examples include: Chef, Puppet, Ansible and SaltStack
 - is convention based for common IAC tasks like file 
   layout, secrets mgmt, etc 
 - idempotence: code that runs correctly no matter how many 
   times its run\re-run
 - intended for distributed networks of remote machines
----

----
Server Templating Tools
 - IAC as fully self-container snap-shots of a whole machine from OS up
 - examples include: Docker, Packer and Vagrant
 - developed the idea of immutable infrastructure 
  - don't change images, just make a whole new one
 - virtual machines and containers
  - a VM is virtualizing the very hardware 
  - container is virtualizing just the user space
   - user space being the part of virtual memory used by applications
   - in contrast to kernel space being the part of the virtual memory
     used by the kernel
----

----
Orchestration Tools
 - IAC as a configuration management tool of all the template servers
 - examples include: Kubernetes, Marathon/Mesos, Amazon Elastic Container 
   Service (Amazon ECS), Docker Swarm and Nomad
 - handle the updates to existing templates in a rolling fashion
  - blue-green deployment
  - canary deployment
 - auto-healing: idea of an orchestration tool replacing poor performing 
   running templates with new ones
 - auto-scaling: idea of an orchestration tool adding\removing running 
   templates based on demand-load
 - load-balancing: idea of orchestration tool distributing demand-load 
   efficiently across the running templates
 - service discovery: idea of orchestration tool allowing for running 
   templates to find and communicate with other running templates
----

----
Provisioning Tools
 - IAC as creation the infrastructure itself
 - examples include: Terraform, CloudFormation and OpenStack Heat
 - creates the servers, databases, caches, load balancers, queues, 
   monitoring, subnet cfg, firewall settings, routing rules, 
   digital certs, etc.
----

----
Terraform Resource Syntax
 - a first-class, kind-of, type is a 'resource'
 - looks similar to a CSS Rule
 resource "<PROVIDER>_<TYPE>" "<NAME>" {
  [CONFIG ...]
 }
 
 - syntax example:
 resource "aws_instance" "myExample" {
  ami = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
 }
 
 - 'resource' is a keyword
 - "aws_instance" is a naming convention of provider_type
 - "myExample" is the name is something I assign
 - what is within the body is specific config key-value pairs
   for that provider_type
----

----
Terraform Resource Attribute Reference
 - a string interpolation schema
 - a reference to some value defined elsewhere
 - uses a snake-plus-dot notation
  - <PROVIDER_<TYPE> are same as always
  - <NAME> is the name of the resource 
  - <ATTRIBUTE> is some key-value in the body of 
    the resource
 <PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>
 
 - assignment of some value to a reference 
   is accomplished by enclosing it in square-braces
  vpc_security_group_ids = [aws_security_group.instance.id]  
  
  - this would mean some other resource is defined 
    for the 'aws_security_group' 
  - the name of this resource is 'instance'
  - lastly, this resource has config attribute named 'id'
----   

----
Terraform Input Variables
 - allow for programmatic declaration of variable 
   in a .tf file
 - can pass in values of input variables using the '-var' param
   when invoking terraform.exe
 - values can also be given using a naming convention of an
   environment variable 
  - convention is 'TF_VAR_' + variable name
 - can also assign the value of an input 
 - variables may be eval'ed inside of string's within a .tf file 
   using PowerShell'esque string interpolation 
 - input variables use 'variable' keyword
 - output variables use 'output' keyword
 
 variable "my-port-number" {
  description = "Basic example of a programmatic terraform variable"
  type = number
  value = 8080
 }
 
 variable "my_list" {
  description = "A list example"
  type = list(number)
  default = [1,2,3]
 }
 
 variable "my_map" {
  description = "A map (aka assoc. array, hashtable, dictionary) example"
  type = map(string)
  
  default = {
    key1 = "value1"
    key2 = "value2"
    key3 = "value3"
  }
 }

 - here is a complex example where a type is being declared then 
   given a default value
 variable "my_object" {
  description = "An example of an object"
  type = object({
   name = string
   age = number
   tags = list(string)
   enabled = bool
  })
  
  default = {
   name = "value1"
   age = 15
   tags = ["a", "b", "c"]
   enabled = true
  }
 }
 
 - input variables can be referenced using special 'var' syntax within 
   a .tf file
 variable "my_other_variable" {
   description = "yet another way"
   type = number
   value = var.my-port-number
 } 
 
 - this allows for something to be printed after invoking terraform.exe
 output "something_out" {
  value = aws_instance.example.public_ip
  description = "Something useful here"  
 }
----

----
Provider Data
 - this is data defined by provider 
 - examples include Virtual Private Cloud data, subnet data, Amazon 
   Machine Image Ids, IP Address ranges, current user identity, etc.
 - similar syntax to 'resource'
 - body of rule typically defines the filtering used to find the data
 
 data "aws_vpc" "default" {
    default = true
 }
 
 data "aws_subnet_ids" "default" {
    vpc_id = data.aws_vpc.default.id
 }
 
 data "aws_iam_user" "example" {
    user_name = "noFuture.aws.app00"
 }
----

----
Terraform Builtin Functions
 - reference at https://www.terraform.io/docs/language/functions/index.html
 - variety of buildin functions within terraform
 - there are numeric, string, collection, encoding, file, date-time,
   crypto, network and conversion functions
----

----
Declaring a Terraform File as Data
 - example of a bash or PowerShell script file 

 data "template_file" "user_data" {
   template = file("my-script.sh")
   
   #declaring these here allows for string 
   # interpolation _inside_ the script file
   vars = {
   
     #using a combo of implicit and input variables
     server_port = var.server_port
     
     #these assume some data resource with 
     # outputs saved to terraform.tfstate
     db_address = data.terraform_remote_state.db.outputs.address
     db_port = data.terraform_remote_state.db.outputs.port
   }
 }
 
 - now within the content of 'my-script.sh', terraform 
   will perform string interpolation
  - not clear how this would work with PowerShell since
    it uses the same form of interpolation
 
 #!/bin/bash
 
 cat > index.html <<EOF
 <h1>Alive</h1>
 <p>DB address: ${db_address}</p>
 <p>DB port: ${db_port}</p>
 EOF
 
 nohup busybox httpd -f -p ${server_port} &
----

----
Running terraform
 - working directory contains a main.tf file
 - run to download dependencies on given provider 
  - as defined in the main.tf 'provider'
  - will also create various other files and folder in working dir
  - is idempotent so can run whenever
 terraform init
 
 - next is the command 'plan'
  - like a 'WhatIf' parameter in PowerShell
  - determines what terraform _will_ do
  - handles the implicit dependencies so resources can be
    declared in .tf files in any order
  - console output has three ascii chars to indicate
    expected future state
   - '+' means something will be created
   - '-' means something will be removed
   - '~' means something will be modified
   - '-/+' means something will be replaced
 terraform plan -var "my_magic_number=90"
 
 - 'apply' command will push what was planned to the 
   cloud provider
  - within the cloud providers console verify it worked
 terraform apply
 
 - 'output' command can be used to query some value of 
   a defined output variable
 terraform output something_out
 
 - 'destroy' command will remove whatever apply created
 terraform destroy
----

----
Basic AWS Elastic Compute Cloud (EC2) instance example

# port 8080 because any port less than 1024 requires root user privs.
variable "my_server_port" {
    description = "The port used in this example"
    type = number
    default = 8080
}

#define the aws instance of a full web server
resource "aws_instance" "example" {
    
    #this is the Id for Ubuntu 18.04 Amazon Machine Image (AMI)
    ami = "ami-0c55b159cbfafe1f0"
    
    #1 virtual CPU and 1 GB of memory
    instance_type = "t2.micro"
    
    #need to attach a security policy (defined below) to this instance
    # VPC stands for Virtual Private Cloud
    vpc_security_group_ids = [aws_security_group.instance.id]
    
    #a Bash script to act as the web content and server
    # uses terraform's string interpolation to assign the port
    user_data = <<-EOF
              #!/bin/bash
              echo "Alive" > index.html
              nohup busybox httpd -f -p ${var.my_server_port} &
              EOF
    
    #this name will appear in my AWS Console's EC2's Instances list
    tags = {
        Name = "terraform-example"
    }
}

#AWS does not allow in nor out traffic by default
# a security setting is needed and then attached to port 8080
resource "aws_security_group" "instance" {
    name = "terraform-example-instance"
    
    #ingress for traffic coming in (as opposed to egress - traffic going out)
    ingress {
    
        #using a variable reference to assign the port
        from_port = var.my_server_port
        to_port = var.my_server_port
        protocol = "tcp"
        
        #Classless Inter Domain Routing (CIDR) 
        cidr_blocks = ["0.0.0.0/0"]#on any IP
    }
}

#this will print the IP after invoking the 'apply' command
output "public_ip" {

    #NOTE: 'example' is the name _we_ gave the resource above
    value = aws_instance.example.public_ip
    description = "The public IP address of the web server"
}
----

----
Creating an AWS EC2 cluster 
 - involves the idea of launch configuration
 - and the idea of auto-scaling group (ASG)
 - an the idea of a load balancer
  - Application Load Balancer: for HTTP, HTTPS traffic
  - Network Load Balancer: for TCP, UDP and TLS traffic
  - load balancers work as:
    Request -> Listeners -> Listener Rules -> Target Groups

provider "aws" {
  region = "us-east-2"
}

#variable declaration
variable "server_port" {
    description = "The port used in this example"
    type = number
    default = 8080
}
#the part that is copied and distributed to each 
# item in the cluster
resource "aws_launch_configuration" "example" {

  #this was called 'ami' in the 'aws_instance'   
  image_id        = "ami-0c55b159cbfafe1f0"
  instance_type   = "t2.micro"

  #this was called 'vpc_security_group_ids' in 'aws_instance' 
  security_groups = [aws_security_group.instance.id]

  user_data = <<-EOF
              #!/bin/bash
              echo "Hello, World" > index.html
              nohup busybox httpd -f -p ${var.server_port} &
              EOF

  #given immutable nature, first create new, then delete old
  lifecycle {
    create_before_destroy = true
  }
}

#define the actual group with its upper and lower limits
resource "aws_autoscaling_group" "example" {

  #use an implicit reference (again, 'example' was what we named it)
  launch_configuration = aws_launch_configuration.example.name

  #subnet ids, distributes the group across various availability zones
  # so that if a data-center goes down the others are still up
  vpc_zone_identifier  = data.aws_subnet_ids.default.ids

  #the load balancer's target group
  target_group_arns = [aws_lb_target_group.asg.arn]

  #this overrides the default health check of "EC2"
  # "ELB" is better since it uses the target group's health check
  health_check_type = "ELB"

  min_size = 2
  max_size = 10

  tag {
    key                 = "Name"
    value               = "terraform-asg-example"
    propagate_at_launch = true
  }
}

#the security group for the launch configuration
resource "aws_security_group" "instance" {
  name = "my-security-group-A"

  ingress {
    from_port   = var.server_port
    to_port     = var.server_port
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

#boilerplate code for subnet ids
data "aws_vpc" "default" {
  default = true
}

#more boilerplate code for subnet ids
data "aws_subnet_ids" "default" {
  vpc_id = data.aws_vpc.default.id
}

#define the load balancer itself
resource "aws_lb" "example" {

  name               = "my-lb"

  load_balancer_type = "application"

  #continuation of AWS EC2 cluster examples above
  subnets            = data.aws_subnet_ids.default.ids

  #defined below, required to allow traffic
  security_groups    = [aws_security_group.alb.id]
}

#define the listener for the load balancer
resource "aws_lb_listener" "http" {

  #this implicit reference to the resource defined above
  load_balancer_arn = aws_lb.example.arn
  port              = 80
  protocol          = "HTTP"

  #by default return 404
  default_action {
    type = "fixed-response"

    fixed_response {
      content_type = "text/plain"
      message_body = "404: page not found"
      status_code  = 404
    }
  }
}

#define the target group for the load balancer
resource "aws_lb_target_group" "asg" {

  name = "my-lb-target-group"

  port     = var.server_port
  protocol = "HTTP"
  vpc_id   = data.aws_vpc.default.id

  #used to check each member of the ASG
  health_check {
    path                = "/"
    protocol            = "HTTP"
    matcher             = "200"
    interval            = 15
    timeout             = 3
    healthy_threshold   = 2
    unhealthy_threshold = 2
  }
}

#define the listener rule for the load balancer
resource "aws_lb_listener_rule" "asg" {

  #"http" is the name given to our aws_lb_listener above
  listener_arn = aws_lb_listener.http.arn
  priority     = 100

  condition {
    path_pattern {
      values = ["*"]
    }
  }

  action {
    type             = "forward"
    #another implicit reference 
    target_group_arn = aws_lb_target_group.asg.arn
  }
}

#security for the load balancer itself, previous security group was for
#each item in the cluster
resource "aws_security_group" "alb" {

  name = "my-security-group-B"

  # Allow inbound HTTP requests
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Allow all outbound requests
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

#need to print the output DNS name
output "alb_dns_name" {
   value = aws_lb.example.dns_name
   description = "The domain name of the load balancer"
}
----

----
Terraform State management
 - each run of Terraform records its state in a file 
   named 'terraform.tfstate'
 - this state file is known as 'backend'
 - .tfstate file is JSON
 - output of the 'plan' command is the diff of .tf files
   and actual state on cloud provider
 - two kinds of backend
  - Local Backend
   - the default of saving .tfstate file to working directory
   - doesn't work for a team-setting
  - Remote Backend
   - these are within the cloud providers
   - examples include: Azure Storage, Google Cloud Storage, 
     Amazon Simple Storage Service (S3), etc.
----

----
Setting up Remote Backend with Amazon S3
 - this should be performed in clean directory 
   with no prior .tfstate files
 - a call to 'terraform init' is required to download
   the dependencies

#specify the provider 
provider "aws" {
  region = "us-east-2"
}

#specify the remote storage place
resource "aws_s3_bucket" "terraform_state" {
  
  #this is globally unique name for _all_ AWS
  bucket = "nofuture-aws-s3-terraform-bucket"
  
  #this is a kind-of lock, invoking 'terraform destroy' 
  # will error out
  lifecycle {
    prevent_destroy = true
  }
  
  #this needs to be enabled so that the remote backend 
  # .tfstate files are versioned
  versioning {
    enabled = true
  }
  
  #enable server-side encryption 
  # this means the file is encrypted on the remote drive
  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"
      }
    }
  }
}

#specify the manner of remote storage file locking
resource "aws_dynamodb_table" "terraform_locks" {
  name = "nofuture-aws-dynamodb-terraform-locks"
  billing_mode = "PAY_PER_REQUEST"
  
  #this hash key value must match this (case-sensitive)
  hash_key = "LockID"
  
  #every 'hash_key' must also be defined as an attribute
  #type is 'S' for string, 'N' for number and 'B' for binary
  attribute {
    name = "LockID"
    type = "S"
  }
}
----

----
Configure Terraform to Store State in S3 Bucket
 - this must be run after the previous 
   section (i.e. 'Setting up Remote Backend with Amazon S3')
   has already been applied
 - previous step was to create resources needed for 
   Remote Backend 
 - this step is configuring terraform to use the Remote Backend

#specify the provider 
provider "aws" {
  region = "us-east-2"
}  

#need to config terraform to use Remote Backend 
# NOTE: no variables can be used here
terraform {
  backend "s3" {
    
    #implicit reference to the S3 bucket defined above
    bucket = "nofuture-aws-s3-terraform-bucket"
    
    #file path within the S3 bucket
    key = "global/s3/terraform.tfstate"
  
    #also needs to match the S3 bucket defined above
    region = "us-east-2"
    
    #implicit reference to the dynamodb table defined above
    dynamodb_table = "nofuture-aws-dynamodb-terraform-locks"
    
    #this is a second layer of encryption, .tfstate file 
    # is encrypted, in addition to the S3 bucket being encrypted
    encrypt = true
  }
}
----